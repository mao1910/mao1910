<!-- VISITOR BADGE -->
<!-- https://github.com/hehuapei/visitor-badge -->

<img align="right" src="https://visitor-badge.laobi.icu/badge?page_id=mao1910.mao1910&left_color=%2379DAF9&right_color=%23FE6E96" />


<!-- TYPING SVG -->
<!-- https://github.com/DenverCoder1/readme-typing-svg -->

<h1 align="center">
    <img src="https://readme-typing-svg.herokuapp.com/?font=Righteous&size=35&center=true&vCenter=true&width=500&height=70&color=FE6E96&font=poppins&duration=5000&lines=Hi+There!+üëã;+I'm+Mao!;" />
</h1>

<br/>

<!-- CODE/TERMINAL ABOUT ME -->
<h1 align="center">
<img src="./assets/terminal-5.gif" alt="Terminal" />
</h1>

<br/><br/><br/>


<!-- TECHNOLOGIES LOGOS -->
<!-- https://github.com/tandpfun/skill-icons -->

<h2 align="center">üíª Languages / Frameworks / Tools ‚öíÔ∏è</h2>
<div align="center">
    <img src="https://skillicons.dev/icons?i=javascript,typescript,angular,react,html,css,scss,bootstrap,cs,java,spring" />
    <img src="https://skillicons.dev/icons?i=flutter,firebase,supabase,mysql,git,github,gitlab,vscode,idea,maven,figma" />
</div>

<br/><br/><br/>


<!-- CONTRIBUTIONS SNAKE GAME -->
<!-- https://github.com/Platane/snk -->

<div align="center">
  <h2> My Contributionssssüêç </h2>
  <br>
  <img alt="contributions-eating Snake" src="https://raw.githubusercontent.com/mao1910/mao1910/output/github-contribution-grid-snake.svg" />

  <!-- Four lines below suggested by Planate for Dark mode-->
  <picture>
  <source media="(prefers-color-scheme: dark)" srcset="github-snake-dark.svg" />
  <source media="(prefers-color-scheme: light)" srcset="github-snake.svg" />
  </picture>
  
  <br/><br/><br/>
</div>


<!-- GITHUB STATS -->
<!-- https://github.com/DenverCoder1/github-readme-streak-stats -->
<!-- https://github.com/anuraghazra/github-readme-stats -->
<!-- https://github-readme-stats-mao1910.vercel.app/ My own Vercel deployment-->

<h2 align="center"> Statsüìù </h2>
  <br>
<div align=center>
  <img width=429 src="https://github-readme-stats-mao1910.vercel.app/api?username=mao1910&count_private=true&show_icons=true&theme=dracula&rank_icon=github&hide=contribs&border_radius=10&border_color=79DAF9" alt="github stats"/>
  <img width=396 src="https://streak-stats.demolab.com/?user=mao1910&count_private=true&theme=dracula&currStreakNum=79DAF9&currStreakLabel=FE6E96&border_radius=10&border=79DAF9" alt="streak stats"/>
  <br/>
  <img src="https://github-readme-stats-mao1910.vercel.app/api/top-langs/?username=mao1910&layout=compact&theme=dracula&border_radius=10&size_weight=0.5&count_weight=0.5&border_color=79DAF9" alt="languages stats" />
</div>

<br/><br/><br/>


<!-- FOOTER -->
<!-- https://github.com/DenverCoder1/readme-typing-svg -->
<!-- https://readme-typing-svg.demolab.com/demo/ -->

<a href="https://git.io/typing-svg"><img src="https://readme-typing-svg.demolab.com?font=Poppins&pause=1000&color=FE6E96&width=535&lines=Thanks+for+dropping+by!;Feel+free+to+check+any+of+the+Socials+below+%F0%9F%91%87;Or+the+Joke+Of+The+Day+if+you're+down+for+a+giggle+%F0%9F%98%9D;Hope+to+see+you+again+%F0%9F%91%8A;Uh%3F+You're+still+here%3F;Well...+I'm+running+out+of+things+to+say...;Tell+you+what%2C+due+to+your+effort+and+perseverance%2C;I+shall+present+you+with+a+short+poem%3A;%22To+code%2C+or+not+to+code%2C+that+is+the+question%3A;Whether+'tis+nobler+in+the+IDE+to+debug;The+errors+and+issues+of+outrageous+software%2C;Or+to+take+up+the+keyboard+against+a+sea+of+bugs;And+by+coding%2C+end+them.%22;by+William+Shakespeare%2C+probably.+;Pretty+sure+that's+Hamlet's.;Alrighty%2C+this+has+been+fun.;But+I'll+restart+the+loop+now...+see+ya+soon!" alt="Typing SVG" /></a>


<!--  SOCIAL NETWORKS -->
<!-- https://github.com/alexandresanlim/Badges4-README.md-Profile -->

  <div> 
    <a href="https://www.linkedin.com/" target="_blank"><img src="https://img.shields.io/badge/-LinkedIn-%230077B5?style=for-the-badge&logo=linkedin&logoColor=white" target="_blank"></a> <!-- ADD LINKEDIN PROFILE -->
    <a href = "https://www.google.com"><img src="https://img.shields.io/badge/Portfolio-4285F4?style=for-the-badge&logo=Google-chrome&logoColor=white" target="_blank"></a> <!-- ADD PORTFOLIO WEBSITE -->
    <a href="https://discord.gg" target="_blank"><img src="https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white" target="_blank"></a> <!-- ADD DISCORD -->
    <a href = "mao1910dev@gmail.com"><img src="https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&logo=gmail&logoColor=white" target="_blank"></a>
  </div>


<!-- SPOTIFY PLAYING-->
<!-- https://github.com/novatorem/novatorem -->
<!-- https://spotify-now-playing-novatorem-git-main-mao1910.vercel.app/ My own Vercel deployment-->

[<img width=438px src="https://spotify-now-playing-git-main-mao1910.vercel.app//api/spotify/?border_color=FE6E96" alt="Mao Spotify Now Playing" />](https://open.spotify.com/user/31542et242zglhf42ydrtqgvuvde)


<!-- JOKE OF THE DAY -->
<!-- https://github.com/ABSphreak/readme-jokes -->
<!-- https://readme-jokes-git-master-mao1910.vercel.app/ My own Vercel deployment-->

<details>
<summary>I've got a Joke for you. Wanna hear it? üôà</summary>

<br/>

 <tr>
 <td style="padding-top:4px"><img src = "https://readme-jokes-git-master-mao1910.vercel.app/api?&theme=dracula"></td>
 </tr>

</details>


<!-- ACTIVITY -->
<!-- https://github.com/jamesgeorge007/github-activity-readme -->


<details>
<summary>‚úçÔ∏è Activity</summary>

<br/>
<!-- START_SECTION:activity -->
<!--END_SECTION:activity-->

</details>


<!-- RSS FEED -->
<!-- https://github.com/gautamkrishnar/blog-post-workflow -->


<details>
<summary>üìï &nbsp;RSS feed</summary>

<br/>

<!-- BLOG-POST-LIST:START -->
 #### - [AWS open source newsletter, #173](https://dev.to/aws/aws-open-source-newsletter-173-3bof) 
 <details><summary>Article</summary> <h2>
  
  
  September 11th, 2023 - Instalment #173
</h2>

<p>Welcome to #173 of the AWS open source newsletter, bringing you all the news and latest projects for AWS developers. This weeks new projects include a Golang based SDK for kernel eBPF operations, a project that helps you to optimise your network performance, a couple of projects for Apache Flink users, as well as a handful of different tools and demos featuring open source technologies helping to drive innovation in generative AI. !As well as the new projects, we also have content this week on open source technologies including ebpf, Apache Flink, Griptape, AWS Amplify, Amazon Corretto, Smithy, lakeFS, Jupyter, GitLab, OpenSearch, Apache Kafka, Apache Iceberg, OpenQAOA, AWS Toolkit for Visual Studio, Apache Airflow, PostgreSQL, MySQL, AWS SAM, PyTorch, and Flux. </p>

<p>Finally, be sure to check out the events section as there are a few events happening this week. Before you dive into the newsletter, check out the following information on open source mentorship program being sponsored by the OpenSearch project.</p>

<p><strong>Open Source Mentorship</strong></p>

<p>The OpenSearch group is going to be doing their second cohort of our open source mentorship program. This program helps new developers make contributions to open source software, giving them a portfolio to launch them into their career. This is a really exciting opportunity, so please make some time to read <a href="https://aws-oss.beachgeek.co.uk/38p">Receive mentorship from Amazon engineers and accelerate your career in Tech</a> where Iskander Rakhman talks about the history of the program, provides lots of details you will want to know, and provides a link where you can sign up. Please share with anyone you know who is looking for an opportunity like this to kick start their open source journey.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--3UiWUaZj--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://opensearch.org/assets/media/blog-images/2023-09-05-college-contributor-initiative/Hot%2520Air%2520Balloon.jpg" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--3UiWUaZj--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://opensearch.org/assets/media/blog-images/2023-09-05-college-contributor-initiative/Hot%2520Air%2520Balloon.jpg" alt="image of open source mentorship blog post" width="800" height="369"></a></p>

<p><strong>Feedback</strong></p>

<p>Before you dive in however, I need your help!  Please please please take 1 minute to <a href="https://www.pulse.aws/promotion/10NT4XZQ">complete this short survey</a> and you will forever have my gratitude! </p>

<h3>
  
  
  Celebrating open source contributors
</h3>

<p>The articles and projects shared in this newsletter are only possible thanks to the many contributors in open source. I would like to shout out and thank those folks who really do power open source and enable us all to learn and build on top of what they have created.</p>

<p>So thank you to the following open source heroes:  Jay Pillai, Shikhar Kwatra, Karthik Sonti,  Ken Collins, Supratip Banerjee, Nathan Peck, Lionel Tchami, Dr. Aparna Sundar, Will Childs-Klein, Andrew Foss, Vijay Karumajji, Eric Johnson, Rio Astamal,  Sukhpreet Bedi, Betty Zheng, and Iskander Rakhman</p>

<h3>
  
  
  Latest open source projects
</h3>

<p><em>The great thing about open source projects is that you can review the source code. If you like the look of these projects, make sure you that take a look at the code, and if it is useful to you, get in touch with the maintainer to provide feedback, suggestions or even submit a contribution. The projects mentioned here do not represent any formal recommendation or endorsement, I am just sharing for greater awareness as I think they look useful and interesting!</em></p>

<h4>
  
  
  Tools
</h4>

<p><strong>hypergraph-tabular-lm</strong></p>

<p><a href="https://aws-oss.beachgeek.co.uk/38l">hypergraph-tabular-lm</a> This repository contains the official implementation for the paper <a href="https://aws-oss.beachgeek.co.uk/38m">HyTrel: Hypergraph-enhanced Tabular Data Representation Learning</a> with code, data, and checkpoints. From the abstract we can see:</p>

<blockquote>
<p>Language models pretrained on large collections of tabular data have demonstrated their effectiveness in several downstream tasks. However, many of these models do not take into account the row/column permutation invariances, hierarchical structure, etc. that exist in tabular data. To alleviate these limitations, we propose HYTREL, a tabular language model, that captures the permutation invariances and three more structural properties of tabular data by using hypergraphs‚Äìwhere the table cells make up the nodes and the cells occurring jointly together in each row, column, and the entire table are used to form three different types of hyperedges.</p>
</blockquote>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--clyksqDQ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://github.com/awslabs/hypergraph-tabular-lm/blob/main/figure1.png%3Fraw%3Dtrue" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--clyksqDQ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://github.com/awslabs/hypergraph-tabular-lm/blob/main/figure1.png%3Fraw%3Dtrue" alt="diagram of hypergraph tabular lm" width="800" height="274"></a></p>

<p><strong>dpdk-setup-eks</strong></p>

<p><a href="https://aws-oss.beachgeek.co.uk/37x">dpdk-setup-eks</a> provides sample code on how you can use packet acceleration using SRIOV (Single Root I/O Virtualization) and DPDK (Data Plane Development Kit) to achieve high network bandwidth, maximum throughput, and minimal latency in your cloud native workloads. SRIOV enables hardware-based acceleration in a virtualised environment that provides higher I/O performance, lower CPU utilisation, higher packet per second (PPS) performance, and lower latency. DPDK provides software-based development kit, which bypasses the operating system (OS) kernel and reduces packet processing overhead, resulting in performance improvement and lower latency. To help you get started with this code, following along with the post <a href="https://aws-oss.beachgeek.co.uk/37y">Automate Packet Acceleration configuration using DPDK on Amazon EKS</a>.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--PX5z_z1---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://d2908q01vomqb2.cloudfront.net/c5b76da3e608d34edb07244cd9b875ee86906328/2023/08/30/Create-EKS-Nodegroup-with-the-pre-built-DPDK-AMI-1024x612.jpg" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--PX5z_z1---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://d2908q01vomqb2.cloudfront.net/c5b76da3e608d34edb07244cd9b875ee86906328/2023/08/30/Create-EKS-Nodegroup-with-the-pre-built-DPDK-AMI-1024x612.jpg" alt="overview of high performance networking on eks" width="800" height="478"></a></p>

<p><strong>aws-ebpf-sdk-go</strong></p>

<p><a href="https://aws-oss.beachgeek.co.uk/38k">aws-ebpf-sdk-go</a> is a Golang based SDK for kernel eBPF operations i.e, load/attach/detach eBPF programs and create/delete/update maps. SDK relies on Unix bpf() system calls. This SDK currently supports eBPF program types (a. Traffic Classifiers b. XDP c. Kprobes/Kretprobes d. Tracepoint probes), and Ring buffer (would need kernel 5.10+). The SDK currently does not support Map in Map and Perf buffer. This is the first version of SDK and interface is subject to change so kindly review the release notes before upgrading.</p>

<p><strong>static-checker-flink</strong></p>

<p><a href="https://aws-oss.beachgeek.co.uk/38n">static-checker-flink</a> The goal of this project is to catch certain issues with Apache Flink applications fast (during build/packaging). Covered cases include Kinesis connector compatibility issues, Apache Kafka connector compatibility issues, and MSK IAM Auth library issues. As an example of how you might use this, did you know that you have to use AWS Kinesis Connector 1.15.4 or above for Apache Flink 1.15 apps? This plugin is there to stop you from building an app that has such incompatible connector versions.</p>

<p><strong>managed-service-for-apache-flink-blueprints</strong></p>

<p><a href="https://aws-oss.beachgeek.co.uk/38o">managed-service-for-apache-flink-blueprints</a> are a curated collection of Apache Flink applications. Each blueprint will walk you through how to solve a practical problem related to stream processing using Apache Flink. These blueprints can be leveraged to create more complex applications to solve your business challenges in Apache Flink, and they are designed to be extensible. We will feature examples for both the DataStream and Table API where possible.</p>

<p>Currently the repo contains two blueprints, and you will find examples of Apache Flink applications that can be run locally, on an open source Apache Flink cluster, or on Managed Service for Apache Flink cluster.</p>

<h3>
  
  
  Demos, Samples, Solutions and Workshops
</h3>

<p><strong>generative-ai-demo-on-miro</strong></p>

<p><a href="https://aws-oss.beachgeek.co.uk/38g">generative-ai-demo-on-miro</a> is the source code for a super cool demo that shows three Generative AI use-cases integrated into single solution on Miro board (digital whiteboard). It turns Python notebooks into dynamic interactive experience, where several team members can brainstorm, explore, exchange ideas empowered by privately hosted Sagemaker generative AI models. This demo can be easily extended by adding use-cases to demonstrate new concepts and solutions.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--fM1D1ox0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_800/https://github.com/aws-samples/generative-ai-demo-on-miro/blob/main/media/genai-demo-960x540_low_fps.gif%3Fraw%3Dtrue" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--fM1D1ox0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_800/https://github.com/aws-samples/generative-ai-demo-on-miro/blob/main/media/genai-demo-960x540_low_fps.gif%3Fraw%3Dtrue" alt="demo of generative miro board" width="" height=""></a></p>

<p><strong>lambda-rag</strong></p>

<p><a href="https://aws-oss.beachgeek.co.uk/38h">lambda-rag</a> is  a Retrieval Augmented Generation Chat AI Demo from AWS Hero Ken Collins. This OpenAI based RAG chat application that can help you learn about AI retrieval patterns. The technologies here are beginner friendly and easy to deploy to AWS Lambda. You will need an OpenAI API key to run this application, so check out the README for more details on other dependencies.</p>

<p>To help you get started and help explain everything,  Ken has put together a couple of great blog posts that really do a fantastic job of explaining the approach and the details. Make sure you read <a href="https://aws-oss.beachgeek.co.uk/38j">RAGs To Riches - Part #1 Generative AI &amp; Retrieval</a>, and the not surprisingly named, <a href="https://aws-oss.beachgeek.co.uk/38i">RAGs To Riches - Part #2 Building On Lambda</a></p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--bRnT2h2w--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://raw.githubusercontent.com/metaskills/lambda-rag/main/public/lambda-rag-hats-light.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--bRnT2h2w--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://raw.githubusercontent.com/metaskills/lambda-rag/main/public/lambda-rag-hats-light.png" alt="example demo screenshot of rag demo app" width="800" height="514"></a></p>

<p><strong>griptape-hello-world</strong></p>

<p><a href="https://aws-oss.beachgeek.co.uk/38e">griptape-hello-world</a> Griptape is an open source project that provides an enterprise grade alternative to tools like LangChain, and in this repo I share some code that I put together whilst testing it out as part of writing a short blog post on this project, <a href="https://aws-oss.beachgeek.co.uk/38f">Getting gnarly with AI - a quick look at Griptape, an enterprise ready alternative to LangChain</a>. Let me know what you think if you try this out.</p>

<p><strong>genai-jumpstart-amplify-cdk-app</strong></p>

<p><a href="https://aws-oss.beachgeek.co.uk/383">genai-jumpstart-amplify-cdk-app</a> In this project we show you how you can take a SageMaker Generative AI model, expose it as a SageMaker Endpoint and consume the Foundational Model in a React Amplify front end. This sample project also demonstrates an implementation of Retrieval Augmented Generation using AWS OpenSearch. The project illustrates how to take sample documents and use a SageMaker Endpoint running an Embeddings LLM to get the embeddings and create an embeddings index within OpenSearch. The app integrates with Cognito for authentication. All the backend components including Lambda, SageMaker Endpoints, OpenSearch, Fargate all run within a VPC. </p>

<p>Jay Pillai, Shikhar Kwatra, and Karthik Sonti have put together a detailed blog post, <a href="https://aws-oss.beachgeek.co.uk/384">Build a secure enterprise application with Generative AI and RAG using Amazon SageMaker JumpStart</a>, to help bootstrap you with this code.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--wBPu8eJg--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2023/08/23/genai-1.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--wBPu8eJg--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2023/08/23/genai-1.png" alt="overview of genai amplify demo app architecture" width="800" height="590"></a></p>

<h3>
  
  
  AWS and Community blog posts
</h3>

<p><strong>Community round up</strong></p>

<p>The community round up is my favourite part of the newsletter, as I get to read about some of the great open source work being done by the AWS community. This week starts off with AWS Community Builder Supratip Banerjee who takes a look at how to build Data version control using lakeFS, an open-source project that provides format-agnostic version control for data lakes, in the post <a href="https://aws-oss.beachgeek.co.uk/38a">A Step-by-Step Guide to Implementing Data Version Control</a>. Next up we have Nathan Peck who has put together <a href="https://aws-oss.beachgeek.co.uk/38b">Deploy Jupyter notebook container with Amazon ECS</a>, which is a nice detailed blueprint that shows how you can deploy Jupyter notebooks on Amazon ECS, leveraging underlying infrastructure optimised for AI (AWS Inferentia and AWS Trainium instance types). AWS Community Builders Lionel Tchami takes a look at setting up CI/CD pipelines using GitLab in his post, <a href="https://aws-oss.beachgeek.co.uk/38c">Every Project Deserves its CI/CD pipeline, no matter how small</a>, something I think we can all agree on. To wrap things up this week, we finish with Dr. Aparna Sundar who has put together <a href="https://aws-oss.beachgeek.co.uk/38d">OpenSearch Dashboards: A usability snapshot</a> that takes a look at the approach taken to enhance the user experience for OpenSearch users.</p>

<p><strong>Amazon Corretto</strong></p>

<p>Amazon Corretto Crypto Provider (ACCP)  is a collection of high-performance cryptographic implementations exposed via standard JCA/JCE interfaces, something I have spoken and demoed in the past. It is super cool stuff! I was therefore delighted when I saw Will Childs-Klein's post, <a href="https://aws-oss.beachgeek.co.uk/37z">Accelerating JVM cryptography with Amazon Corretto Crypto Provider 2</a> which looks at the updated version (ACCP 2) delivers comprehensive performance enhancements, with some algorithms (such as elliptic curve key generation) seeing a greater than 13-fold improvement over ACCP 1. This release also sees changes to the backing cryptography library for ACCP from OpenSSL (used in ACCP 1) to the AWS open source cryptography library, AWS libcrypto (AWS-LC). If you are a Java developer, then this is a must read post this week.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--htytlXIK--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://d2908q01vomqb2.cloudfront.net/22d200f8670dbdb3e253a90eee5098477c95c23d/2023/08/29/img1-2.jpg" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--htytlXIK--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://d2908q01vomqb2.cloudfront.net/22d200f8670dbdb3e253a90eee5098477c95c23d/2023/08/29/img1-2.jpg" alt="overview of accp2 vs accp1 benchmark" width="800" height="448"></a></p>

<p><strong>Smithy</strong></p>

<p>Smithy is an open-source Interface Definition Language (IDL) and set of tools for building web services, created by AWS. AWS uses Smithy to model services, generate server scaffolding, generate SDKs for multiple languages, and generate AWS SDKs. Andrew Foss is excited to announce the release of a new capability, which he writes about in his post <a href="https://aws-oss.beachgeek.co.uk/389">Creating Smithy Projects with Smithy Init</a>. The release of the init command in Smithy CLI, enables developers to create new Smithy projects quickly and easily. Jump into the post to find out more about Smithy and this new update. [hands on]</p>

<p><strong>Other posts and quick reads</strong></p>

<ul>
<li>
<a href="https://aws-oss.beachgeek.co.uk/380">Introducing Amazon MSK as a source for Amazon OpenSearch Ingestion</a> looks at Amazon MSK as a source to Amazon OpenSearch Ingestion, a serverless, fully managed, real-time data collector for OpenSearch Service that makes this ingestion even easier [hands on]</li>
</ul>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--3-aFYt6E--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2023/08/22/BDB-3631-image001.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--3-aFYt6E--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2023/08/22/BDB-3631-image001.png" alt="overview of amazon msk and opensearch ingestion" width="800" height="340"></a></p>

<ul>
<li>
<a href="https://aws-oss.beachgeek.co.uk/381">Query your Iceberg tables in data lake using Amazon Redshift (Preview)</a> provides an example of querying an Iceberg table in Redshift using files stored in Amazon S3, demonstrating some of the key features like efficient row-level update and delete, and the schema evolution experience [hands on]</li>
</ul>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--EDk-x2sa--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2023/08/29/BDB-3187-image001.jpg" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--EDk-x2sa--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2023/08/29/BDB-3187-image001.jpg" alt="overview of apache iceberg tables in redshift" width="800" height="827"></a></p>

<ul>
<li>
<a href="https://aws-oss.beachgeek.co.uk/386">Optimization with OpenQAOA on Amazon Braket</a> explores how the open source project OpenQAOA is integrated  with Amazon Braket, demonstrating how to cast an optimisation problem</li>
<li>
<a href="https://aws-oss.beachgeek.co.uk/387">Build streaming data pipelines with Amazon MSK Serverless and IAM authentication</a> shows you how to create a serverless integration Lambda function between API Gateway and MSK Serverless as a way to do IAM authentication when your producer is not written in Java [hands on]</li>
</ul>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--VRwlfd-B--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2023/08/24/image001.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--VRwlfd-B--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2023/08/24/image001.png" alt="overview of Amazon MSK serverless and IAM authentication" width="800" height="353"></a></p>

<ul>
<li>
<a href="https://aws-oss.beachgeek.co.uk/388">Use the reverse token filter to enable suffix matching queries in OpenSearch</a> gives a hands on guide on how you can implement a suffix-based search [hands on]</li>
</ul>

<h3>
  
  
  Quick updates
</h3>

<p><strong>AWS Toolkit for Visual Studio</strong></p>

<p>Announced last week was news that the AWS Toolkit for Visual Studio is now generally available on the Arm64 version of Visual Studio (aka ‚ÄúArm64 Visual Studio‚Äù). This release enables a Visual Studio user on a native Windows Arm64 device or on a device emulating Windows Arm64 on a M class Apple device to leverage the same AWS tooling that has been available to x64 versions of Visual Studio. You can read the full details in the post, <a href="https://aws-oss.beachgeek.co.uk/382">AWS Toolkit for Visual Studio adds support for Arm64 Visual Studio</a></p>

<p><strong>AWS SDK</strong></p>

<p>There is an upcoming change in the S3 GetObjectAttributes API, and so John Viegas has put together a post that users of the AWS SDK for Java v2, AWS SDK for .NET v3, and AWS Tools for PowerShell should read and understand how they can prepare. You can catch the post here, <a href="https://aws-oss.beachgeek.co.uk/385">Update to AWS SDK for Java v2, AWS SDK for .NET v3, and AWS Tools for PowerShell when using S3 GetObjectAttributes API</a></p>

<p><strong>Apache Airflow</strong></p>

<p>Amazon Managed Workflows for Apache Airflow (MWAA) has added certifications for International Organization for Standardization (ISO) and Information Security Registered Assessors Program (IRAP). Amazon Web Services (AWS) maintains certifications through extensive audits of its controls to ensure that information security risks that affect the confidentiality, integrity, and availability of company and customer information are appropriately managed. </p>

<p>Amazon MWAA is a managed orchestration service for Apache Airflow that makes it easier to set up and operate end-to-end data pipelines in the cloud. Amazon MWAA has certification for compliance with ISO/IEC 27001:2013, 27017:2015, 27018:2019, 27701:2019, 22301:2019, 9001:2015, and CSA STAR CCM v4.0. You can download copies of the AWS ISO certificates and use them to jump-start your own certification efforts. Further, with IRAP certification, you can meet the Australian Government Information Security Manual (ISM) control objectives while using Amazon MWAA.</p>

<p>In addition to ISO and IRAP certification, Amazon MWAA is also Health Insurance Portability and Accountability Act (HIPAA) eligible, in scope for System and Organization Controls (SOC) reports, and Payment Card Industry Data Security Standard (PCI) compliant. </p>

<p><strong>MySQL and PostgreSQL</strong></p>

<p>Amazon Relational Database Service (RDS) announces Amazon RDS Extended Support for Amazon Aurora and Amazon RDS database instances running MySQL 5.7, PostgreSQL 11, and higher major versions beyond the community end of life. Amazon RDS Extended Support provides you more time, up to three years, to upgrade to a new major version to help you meet your business requirements. Extended Support is available for Aurora MySQL-compatible edition, Aurora PostgreSQL-compatible edition, RDS for MySQL and RDS for PostgreSQL.</p>

<p>Starting in December, 2023, you will be able to opt-in to Amazon RDS Extended Support through the AWS Console, CLI, and APIs. When you opt-in to Extended Support, Amazon RDS will provide critical security and bug fixes for your MySQL and PostgreSQL databases after the community ends support for a major version. You can run your databases on Amazon Aurora and Amazon RDS with Extended Support for up to three years beyond a major version‚Äôs end of standard support date. Learn more about Extended Support, including supported engine versions, in the Amazon Aurora user guide and Amazon RDS User Guide.</p>

<p>Amazon RDS Extended Support is now available for Aurora MySQL-Compatible version 2 and higher, Aurora PostgreSQL-Compatible version 11 and higher, RDS for MySQL major versions 5.7 and higher, and RDS for PostgreSQL major versions 11 and higher in AWS Commercial and AWS GovCloud (US) Regions.</p>

<p>Find out more by reading the post, <a href="https://aws-oss.beachgeek.co.uk/37w">Introducing Amazon RDS Extended Support for MySQL databases on Amazon Aurora and Amazon RDS</a>, where Vijay Karumajji makes some compelling arguments why you should try and upgrade, but failing that, how  Amazon RDS Extended Support can help.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--8n2jr4FJ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2023/09/01/DBBLOG-3499_img1-1024x308.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--8n2jr4FJ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2023/09/01/DBBLOG-3499_img1-1024x308.png" alt="overview of extended support diagram for MySQL" width="800" height="241"></a></p>

<p><strong>PostreSQL</strong></p>

<p>Aside from the Extended support news just mentioned, there are a couple of other updates worth noting.</p>

<p>First is news that Amazon Relational Database Service (RDS) for PostgreSQL now supports the h3-pg extension, which provides an API to H3, an open-source hexagonal, hierarchical geospatial indexing system. With this extension, you can perform different kinds of spatial analysis over large datasets, including efficient indexing and lookups, modeling flow through a grid, and applying machine learning models over your geospatial data stored in Amazon RDS for PostgreSQL. The H3 library provides an invariant set of hexagonal map tiles over multiple layers of resolution. This allows the h3-pg extension to index your geospatial data so you can efficiently query data on your maps. For example, a retailer planning new outlets may want to create a heatmap visualisation using traffic, mobility, demographic, and other geospatial datasets to identify locations best suited for their customers. You can also use H3 and PostGIS together to perform different geospatial analyses. h3-pg is available on database instances in Amazon RDS running PostgreSQL 15.4, 14.9, 13.12 and higher in all applicable AWS Regions.</p>

<p>Finally, Amazon RDS for PostgreSQL 16 Release Candidate 1 (RC1) is now available in the Amazon RDS Database Preview Environment, allowing you to evaluate the pre-release of PostgreSQL 16 on Amazon RDS for PostgreSQL. You can deploy PostgreSQL 16 RC1 in the Preview Environment and have the same benefits of a fully managed database, making it simpler to set up, operate, and monitor databases. PostgreSQL 16RC1 in the Preview Environment also includes support for logical decoding on read replicas, AWS libcrypto (AWS-LC), and over 80 PostgreSQL extensions such as pgvector, pg_tle, h3-pg, pg_cron, and rdkit.</p>

<p>The PostgreSQL community released PostgreSQL 16 RC1 on August 31, 2023 that enables logical replication from standbys and includes numerous performance improvements. PostgreSQL 16 also adds support for SQL/JSON constructors and identity functions, more query types that can use parallelism, introduction of using SIMD CPU acceleration, and the ‚Äòpg_stat_io‚Äô view that provides statistics on I/O usage. The Amazon RDS Database Preview Environment supports the latest generation of instance classes that are retained for a maximum period of 60 days and are automatically deleted after the retention period. Amazon RDS database snapshots that are created in the Preview Environment can only be used to create or restore database instances within the Preview Environment. You can use the PostgreSQL dump and load functionality to import or export your databases from the Preview Environment.</p>

<p><strong>AWS Serverless Application Model (SAM)</strong></p>

<p>The AWS Serverless Application Model (SAM) Command Line Interface (CLI) announces the launch of SAM CLI local testing and debugging on HashiCorp Terraform. The AWS SAM CLI is a developer tool that makes it easier to build, test, package, and deploy serverless applications. Terraform is an infrastructure as code tool that lets you build, change, and version cloud and on-premises resources safely and efficiently.</p>

<p>Customers can now use the SAM CLI to locally test and debug AWS Lambda functions and Amazon API Gateway defined in their Terraform application. SAM CLI can read the infrastructure resource information from the Terraform project and start Lambda functions and API Gateway endpoints locally running in a Docker container. Customers can invoke their function or API endpoint with an event payload, or attach a debugger using AWS toolkits on IDE to step through the Lambda function code. Previously, SAM CLI only supported local testing and debugging on CloudFormation templates. With this change, Terraform users can use the SAM CLI local testing commands like sam local start-api, sam local start-lambda and sam local invoke on their Terraform projects to speed up their development cycles. They can also use sam local generate command to generate mock test events for local testing.</p>

<p>This feature is supported with Terraform version 1.1+ and you can find out more by reading the post from Eric Johnson, <a href="https://aws-oss.beachgeek.co.uk/37v">AWS SAM support for HashiCorp Terraform now generally available</a></p>

<p><strong>PyTorch</strong></p>

<p>SageMaker Multi-Model Endpoint (MME) is a fully managed capability that allows customers to deploy 1000s of models on a single SageMaker endpoint and reduce costs. Until today, MME was not supported for PyTorch models deployed using TorchServe. Now, customers can use MME to deploy 1000s of PyTorch models using TorchServe to reduce inference costs.</p>

<p>Customers are increasingly building ML models using PyTorch to achieve business outcomes, To deploy these ML models, customers use TorchServe on CPU/GPU instances to meet desired latency and throughput goals. However, costs can add up if customers are deploying 10+ models. With MME support for TorchServe, customers can deploy 1000s of PyTorch based models on a single SageMaker endpoint. Behind the scenes, MME will run multiple models on a single instance and dynamically load/unload models across multiple instances based on the incoming traffic. With this feature, customers can save costs, as they can share instances behind an endpoint across 1000s of models and only pay for the number of instances used. </p>

<p>This feature supports PyTorch models which use SageMaker TorchServe Inference Container with all machine learning optimised CPU instances and single GPU instances in ml.g4dn, ml.g5, ml.p2, ml.p3 family. It is also available in all regions supported by Amazon SageMaker. </p>

<p><strong>lightsail-miab-installer</strong></p>

<p>This is a project from my fellow developer advocate Rio Astamal, that provides a user-friendly command-line tool to streamline the setup of Mail-in-a-Box on Amazon Lightsail. Rio contacted me that <a href="https://aws-oss.beachgeek.co.uk/2ym">lightsail-miab-installer</a> has had an update so go check out the changelog for updates. </p>

<h3>
  
  
  Videos of the week
</h3>

<p><strong>Start building with PL/Rust in Amazon RDS for PostgreSQL</strong></p>

<p>Rust combines the performance and resource efficiency of compiled languages like C with mechanisms that limit the risks from unsafe memory use. As a PostgreSQL trusted procedural language, PL/Rust provides memory safety so that an unprivileged user can run code in the database without the risk of crashing the database due to a software defect that corrupts memory. Developers can also package PL/Rust code as Trusted Language Extensions (TLE) for PostgreSQL to run on Amazon RDS. RDS for PostgreSQL customers can now use Rust to build high performance user defined functions to extend PostgreSQL for compute-intensive data processing. </p>

<p>In this session, Sukhpreet Bedi provides a brief introduction to Rust, walk you through how to deploy RDS for PostgreSQL with PL/Rust enabled, and show you how to write high-performance Rust code directly to your database.</p>

<p><iframe width="710" height="399" src="https://www.youtube.com/embed/ZluZH0Q5Mhw">
</iframe>
</p>

<p><strong>Mastering GitOps with Flux: Step-by-Step Guide for Effective Implementation</strong></p>

<p>GitOps is an effective way to achieve continuous deployment for Kubernetes clusters while meeting enterprise requirements like security, separation of privileges, audibility, and agility. In this series of 4 demos, Betty Zheng will show you some good practices for GitOps based on EKS and Flux CD. Check the YouTube listing for the supporting code so you can follow along too.</p>

<p><iframe width="710" height="399" src="https://www.youtube.com/embed/_jdq7BhK4IQ">
</iframe>
</p>

<p><strong>Open Source Brief</strong></p>

<p>Now featured every week in the AWS Community Radio show, grab a quick five minute recap of the weekly open source newsletter from yours truely.</p>

<p><iframe width="710" height="399" src="https://www.youtube.com/embed/zQjtBsjjCcc">
</iframe>
</p>

<p>Check out the <a href="https://aws-oss.beachgeek.co.uk/359">playlist here</a>.</p>

<p><strong>Build on Open Source</strong></p>

<p>For those unfamiliar with this show, Build on Open Source is where we go over this newsletter and then invite special guests to dive deep into their open source project. Expect plenty of code, demos and hopefully laughs. We have put together a playlist so that you can easily access all (sixteen) of the episodes of the Build on Open Source show. <a href="https://aws-oss.beachgeek.co.uk/episodes">Build on Open Source playlist</a>.</p>

<p>We are currently planning the third series - if you have an open source project you want to talk about, get in touch and we might be able to feature your project in future episodes of Build on Open Source.</p>

<h1>
  
  
  Events for your diary
</h1>

<p>If you are planning any events in 2023, either virtual, in person, or hybrid, get in touch as I would love to share details of your event with readers. </p>

<p><strong>Building ML capabilities with PostgreSQL and pgvector extension</strong><br>
<strong>YouTube, 14th September 4pm UK time</strong></p>

<p>Generative AI and Large Language Models (LLMs) are powerful technologies for building applications with richer and more personalized user experiences. Application developers who use Amazon Aurora for PostgreSQL or Amazon RDS for PostgreSQL can use pgvector, an open-source extension for PostgreSQL, to harness the power of generative AI and LLMs for driving richer user experiences. Register now to learn more about this powerful technology.</p>

<p>Watch it <a href="https://aws-oss.beachgeek.co.uk/325">live on YouTube</a>.</p>

<p><strong>Build ML into your apps with PostgreSQL and the pgvector extension</strong><br>
<strong>YouTube, 21st September 4pm UK time</strong></p>

<p>This office hours session is a follow up for those who attended the fireside chat titled "Building ML capabilities into your apps with PostgreSQL and the open-source pgvector extension". Others are also welcome. Office hours attendees can ask questions related to this topic. Application developers who use Amazon Aurora for PostgreSQL or Amazon RDS for PostgreSQL can use pgvector, an open-source extension for PostgreSQL, to harness the power of generative AI and LLMs for driving richer user experiences. Join us to ask your questions and hear the answers to the most frequently asked questions about the pgvector extension for PostgreSQL.</p>

<p>Watch it <a href="https://aws-oss.beachgeek.co.uk/326">live on YouTube</a>.</p>

<p><strong>Open Source Summit, Europe</strong><br>
<strong>September 19th-21st, Bilboa Spain</strong></p>

<p>"Open Source Summit is the premier event for open source developers, technologists, and community leaders to collaborate, share information, solve problems, and gain knowledge, furthering open source innovation and ensuring a sustainable open source ecosystem. It is the gathering place for open-source code and community contributors." You will find AWS as well as myself at Open Source Summit this year, so come by the AWS booth and say hello - from the glimpses I have seen so far, it is going to be awesome! Find out more at the official site, <a href="https://aws-oss.beachgeek.co.uk/31f">Open Source Summit Europe 2023</a>.</p>

<p><strong>OpenSearchCon</strong><br>
<strong>Seattle, September 27-29, 2023</strong></p>

<p>Registration is now open source OpenSearchCon. Check out this post from Daryll Swager, <a href="https://aws-oss.beachgeek.co.uk/2zk">Registration for OpenSearchCon 2023 is now open!</a> that provides you with what you can expect, and resources you need to help plan your trip.</p>

<p><strong>CDK Day, 2023</strong><br>
<strong>Online, 29th September 2023</strong></p>

<p>Back for the fourth instalment, this Community led event is a must attend for anyone working with infrastructure as code using the AWS Cloud Development Kit (CDK). It is intended to provide learning opportunities for all users of the CDK and related libraries. The event will be live streamed on YouTube, and you check more at the website, <a href="https://aws-oss.beachgeek.co.uk/fr">CDK Day</a> </p>

<p><strong>All Things Open</strong><br>
<strong>October, 15th-17th, Raleigh Convention Center, Raleigh, North Carolina</strong></p>

<p>I will be attending and speaking at All Things Open, looking at Apache Airflow as an container orchestrator. I will be there with a bunch of fellow AWS colleagues, and I hope to meet some of you there. Check us out at the AWS booth, where you will find me and the other AWS folk throughout the event. Check out the event and sessions/speakers at the official webpage for the event, <a href="https://aws-oss.beachgeek.co.uk/31e">AllThingsOpen 2023</a></p>

<p><strong>Open Source India</strong><br>
<strong>October 19th-21st, NIMHANS Convention Center, Bengaluru</strong></p>

<p>One of the most important open source events in the region, Open Source India will be welcoming thousands of attendees all to discuss and learn about open source technologies. I will be there too, doing a talk so I would love to meet with any of you who are also planning on attending. Check out more details on their web page, <a href="https://aws-oss.beachgeek.co.uk/31d">here</a>.</p>

<p><strong>Cortex</strong><br>
<strong>Every other Thursday, next one 16th February</strong></p>

<p>The Cortex community call happens every two weeks on Thursday, alternating at 1200 UTC and 1700 UTC. You can check out the GitHub project for more details, go to the <a href="https://aws-oss.beachgeek.co.uk/2h5">Community Meetings</a> section. The community calls keep a rolling doc of previous meetings, so you can catch up on the previous discussions. Check the <a href="https://aws-oss.beachgeek.co.uk/2h6">Cortex Community Meetings Notes</a> for more info.</p>

<p><strong>OpenSearch</strong><br>
<strong>Every other Tuesday, 3pm GMT</strong></p>

<p>This regular meet-up is for anyone interested in OpenSearch &amp; Open Distro. All skill levels are welcome and they cover and welcome talks on topics including: search, logging, log analytics, and data visualisation.</p>

<p>Sign up to the next session, <a href="https://aws-oss.beachgeek.co.uk/1az">OpenSearch Community Meeting</a></p>

<h3>
  
  
  Stay in touch with open source at AWS
</h3>

<p>Remember to check out the <a href="https://aws.amazon.com/opensource/?opensource-all.sort-by=item.additionalFields.startDate&amp;opensource-all.sort-order=asc">Open Source homepage</a> to keep up to date with all our activity in open source by following us on <a href="https://twitter.com/AWSOpen">@AWSOpen</a></p>

 </details> 
 <hr /> 

 #### - [üñä I am building a pastebin alternative!](https://dev.to/shaancodes/i-am-building-a-pastebin-alternative-57o2) 
 <details><summary>Article</summary> <p>Hi.<br>
I wanted to learn T3 stack as it is trend right now. So what better way is there to learn a particular stack than actually creating a project in it. But, I did not wanted to create the traditional recipe app, social media app or a boring e-commerce app. I wanted to create something which can be used by others also.</p>

<p>So, I thought to myself what are the apps which I use, but it could've been better if I built it. I thought of lot of apps, but later came across pastebin. I thought to myself, "I could build it better".</p>

<h2>
  
  
  Is it a regular pastebin clone?
</h2>

<p>No, it's not exactly a pastebin clone, but rather I'm planning it to build it as an alternative to it. It's going to be better, with beautiful UI and everything. </p>

<h2>
  
  
  How is it going to be different?
</h2>

<p>The difference is I am gonna let users write notion like documents and then share it, unlike pastebin which only lets us share text. For writing documents, I'm gonna use <a href="https://novel.sh/">Novel.sh</a>. It is a WYSIWYG editor which provides interface and features similar to Notion. It also lets us use OpenAI API to integrate AI into it.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--pEW7PWB8--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2v4xi3tkwerp9a7e23yd.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--pEW7PWB8--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2v4xi3tkwerp9a7e23yd.png" alt="Novel.sh" width="800" height="693"></a></p>

<p>It will have all the regular features like document sharing, password protection, document exposure, document expiration etc. For now, I'm planning to build a MVP in a week or so, and later I'll improve it by introducing more features into it.</p>

<h2>
  
  
  What tech stack I'm planning to use?
</h2>

<ul>
<li>T3 stack</li>
<li>Novel.sh (WYSIWYG editor)</li>
<li>shadcn/ui</li>
<li>MySql</li>
<li>PlanetScale</li>
<li>Vercel</li>
</ul>

<p>I am bad at naming things üòÅ, I had hard time thinking about a name for this project. I came up with "docshare", but I think it can be further improved. I'd appreciate it if you guys can help me come up with a perfect name for this project. Thanks in advance!</p>

<h3>
  
  
  My Socials
</h3>

<p>Twitter - <a href="https://twitter.com/shaancodes">https://twitter.com/shaancodes</a></p>

<p>Instagram - <a href="https://www.instagram.com/shaancodes/">https://www.instagram.com/shaancodes/</a></p>

<p>GitHub - <a href="https://github.com/shaan-alam/">https://github.com/shaan-alam/</a></p>

 </details> 
 <hr /> 

 #### - [Which AI Tool is the Best?](https://dev.to/ronakmunjapara/which-ai-tool-is-the-best-1987) 
 <details><summary>Article</summary> <p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--8iWs-JGQ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/fkbjq67b4fmemjbfmf08.jpeg" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--8iWs-JGQ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/fkbjq67b4fmemjbfmf08.jpeg" alt="Image description" width="800" height="800"></a><br>
In today's digital age, the integration of artificial intelligence (AI) has revolutionized various industries. From healthcare to finance, AI has proven to be a game-changer. But when it comes to choosing the best AI tool, the landscape can be overwhelming. With a multitude of options available, it's essential to navigate the terrain wisely to select the one that aligns with your specific needs. In this comprehensive guide, we'll delve into the world of AI tools, dissecting their features, applications, and advantages, to help you make an informed decision.</p>

<h2>
  
  
  Understanding the AI Ecosystem
</h2>

<p>Before we embark on our journey to uncover the best AI tool, it's crucial to understand the diverse AI ecosystem. Artificial intelligence encompasses a wide range of technologies and applications, each designed to address specific challenges and tasks. Here are some key components of the AI landscape:</p>

<h3>
  
  
  Machine Learning
</h3>

<p>Machine learning is the foundation of many AI tools. It involves training algorithms to learn from data and make predictions or decisions without explicit programming. Machine learning finds applications in image recognition, natural language processing, and recommendation systems.</p>

<h3>
  
  
  Natural Language Processing (NLP)
</h3>

<p>NLP focuses on enabling computers to understand, interpret, and generate human language. It powers chatbots, virtual assistants, and language translation tools.</p>

<h3>
  
  
  Computer Vision
</h3>

<p>Computer vision enables machines to interpret and process visual information from the world. It plays a vital role in facial recognition, autonomous vehicles, and medical image analysis.</p>

<h3>
  
  
  Robotics
</h3>

<p>AI-driven robotics involve the development of intelligent machines that can perform tasks autonomously or semi-autonomously. Industrial automation and healthcare are prominent domains for AI robotics.</p>

<h3>
  
  
  Deep Learning
</h3>

<p>Deep learning is a subset of machine learning that involves neural networks with multiple layers. It's used for complex tasks like voice recognition, image analysis, and autonomous decision-making.</p>

<h3>
  
  
  AI in Business
</h3>

<p>AI has gained prominence in the business world, with applications ranging from predictive analytics to customer relationship management. Businesses use AI to enhance operations, improve customer experiences, and drive innovation.</p>

<h2>
  
  
  The Quest for the Best AI Tool
</h2>

<p>Now that we've gained a foundational understanding of AI, let's embark on the quest to find the best AI tool. It's important to note that the "best" AI tool varies depending on your specific needs and goals. What suits one organization or individual may not be the optimal choice for another. To determine the best AI tool for you, consider the following factors:</p>

<h3>
  
  
  1. <strong>Use Case</strong>
</h3>

<p>Define your specific use case. Are you looking for an AI tool to automate customer support, analyze large datasets, or enhance your creative projects? Understanding your use case is the first step in narrowing down your options.</p>

<h3>
  
  
  2. <strong>Scalability</strong>
</h3>

<p>Consider the scalability of the AI tool. Will it accommodate your needs as your requirements grow? Scalability is crucial for businesses that anticipate expansion.</p>

<h3>
  
  
  3. <strong>Ease of Integration</strong>
</h3>

<p>How easily can the AI tool integrate with your existing systems and workflows? Seamless integration can save time and resources.</p>

<h3>
  
  
  4. <strong>Accuracy and Performance</strong>
</h3>

<p>Assess the accuracy and performance of the AI tool in your chosen domain. Look for user reviews, case studies, and performance benchmarks.</p>

<h3>
  
  
  5. <strong>Cost and ROI</strong>
</h3>

<p>Evaluate the cost of implementing the AI tool and weigh it against the potential return on investment (ROI). Consider both short-term and long-term expenses.</p>

<h3>
  
  
  6. <strong>Support and Training</strong>
</h3>

<p>Check whether the AI tool provider offers adequate support, training, and documentation. A robust support system can be invaluable during implementation.</p>

<h3>
  
  
  7. <strong>Security and Compliance</strong>
</h3>

<p>For businesses, security and compliance are paramount. Ensure that the AI tool aligns with your data security and regulatory requirements.</p>

<h2>
  
  
  Top Contenders in the AI Arena
</h2>

<p>While the best AI tool ultimately depends on your specific needs, we can explore some top contenders that have garnered acclaim in various domains:</p>

<h3>
  
  
  1. <strong>OpenAI's GPT-3</strong>
</h3>

<p>GPT-3, powered by OpenAI, is a language processing AI model known for its ability to generate human-like text. It's utilized in content generation, chatbots, and language translation.</p>

<h3>
  
  
  2. <strong>IBM Watson</strong>
</h3>

<p>IBM Watson offers a suite of AI solutions for businesses, including AI-powered analytics, natural language processing, and automation tools. It's renowned for its versatility and scalability.</p>

<h3>
  
  
  3. <strong>Google Cloud AI</strong>
</h3>

<p>Google Cloud AI provides a range of AI services, from machine learning APIs to AI platform tools. It's a favorite among businesses looking to leverage Google's extensive AI capabilities.</p>

<h3>
  
  
  4. <strong>Amazon Web Services (AWS) AI</strong>
</h3>

<p>AWS AI offers a comprehensive set of AI and machine learning services, including computer vision, natural language processing, and predictive analytics.</p>

<h3>
  
  
  5. <strong>Microsoft Azure AI</strong>
</h3>

<p>Microsoft Azure AI provides a suite of AI tools for developers and businesses, with offerings in machine learning, computer vision, and speech recognition.</p>

<h2>
  
  
  Conclusion
</h2>

<p>The quest for the best AI tool is an ongoing journey, as the field of artificial intelligence continues to evolve. To make an informed choice, start by clearly defining your use case and objectives. Consider scalability, ease of integration, performance, cost, and support. Finally, explore the top contenders in the AI landscape to find the tool that aligns with your unique needs. The best AI tool is the one that empowers you to achieve your goals and unlock the full potential of artificial intelligence.</p>

 </details> 
 <hr /> 

 #### - [Let‚Äôs Create an End-to-End Web Scraping Pipeline With Scrapy!](https://dev.to/nanellooo/lets-create-an-end-to-end-web-scraping-pipeline-with-scrapy-261o) 
 <details><summary>Article</summary> <h1>
  
  
  Introduction
</h1>

<p>Web scraping has become an indispensable tool for gathering data, allowing developers and data enthusiasts access to valuable information from the web. Tools like BeautifulSoup4 and Selenium are user-friendly tools that make this task as simple as possible, especially for one-off scripts and basic workflows.</p>

<p>However, web scraping is often just the first step in a broader process of Extract, Transform, Load (ETL). As your needs grow, so too will the number of custom scripts. With no framework to organize these one-off scripts, this will inevitably lead to a confusing mess down the road.</p>

<p>As Albert Einstein once said, ‚ÄúEverything should be made as simple as possible, but not simpler.‚Äù That‚Äôs where Scrapy comes in!</p>

<p>In this tutorial, I‚Äôm going to walk you through a web scraping ETL process using Scrapy that gathers quotes, like that Einstein quote, and loads them into an SQLite database. We‚Äôll be using <a href="http://quotes.toscrape.com/page/1/">Quotes to Scrape</a> as our target scraping site:</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--IP00BGJG--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/sk0edksszz2lp0ett6nd.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--IP00BGJG--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/sk0edksszz2lp0ett6nd.png" alt="Image description" width="800" height="494"></a></p>

<h1>
  
  
  We‚Äôll go over the following:
</h1>

<ul>
<li>  Creating a virtual environment for Python.</li>
<li>  Setting up Scrapy, a Python web scraping framework.</li>
<li>  Building a web scraper using Scrapy to extract quotes from a website.</li>
<li>  Configuring a Scrapy pipeline to process and store scraped data.</li>
<li>  Creating a SQLite database using Python.</li>
<li>  Storing scraped data in an SQL database.</li>
<li>  (For Fun) Analyzing scraped data with Pandas and Matplotlib</li>
</ul>

<h1>
  
  
  Step 1: Creating a Virtual Environment
</h1>

<p>Before we dive in, a good idea to create a clean and isolated Python environment using a virtual environment. This ensures that the packages and dependencies for your Scrapy project won‚Äôt interfere with your system-wide Python installation, and it will automatically Scrapy‚Äôs CLI to the virtual environment‚Äôs PATH for maximum ease of use.</p>

<p>Here are the steps to create a virtual environment:</p>

<ol>
<li> Open a terminal or command prompt on your computer.</li>
<li> Navigate to the directory where you want to create your Scrapy project. You</li>
<li> Once you‚Äôre in the desired directory, run the following command to create a virtual environment named <code>quotesenv</code> (you can replace <code>quotesenv</code> with your preferred name):
</li>
</ol>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>python -m venv quotesenv
</code></pre>

</div>



<p>After running this command, you‚Äôll have a new directory named <code>myenv</code> (or your chosen name) in your project directory. This directory contains a clean Python environment where you can install packages without affecting your system-wide Python installation.</p>

<p>Now that you have a virtual environment set up, you can proceed to the next step: installing Scrapy and creating your Scrapy project.</p>

<h1>
  
  
  Step 2: Installing Scrapy and Creating Your First Project
</h1>

<p>It‚Äôs time to install Scrapy and create a Scrapy project for our web scraping endeavor.</p>

<ol>
<li> Activate your virtual environment if it‚Äôs not already activated. You can do this by running:
</li>
</ol>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>source quotesenv/bin/activate
</code></pre>

</div>



<p>Replace <code>quotesenv</code> with the name of your virtual environment if it's different.</p>

<ol>
<li> Now, you can install Scrapy inside your virtual environment using pip:
</li>
</ol>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>pip install scrapy
</code></pre>

</div>



<ol>
<li> Once Scrapy is installed, you can create a new Scrapy project using the following command:
</li>
</ol>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>scrapy startproject quotes\_project
</code></pre>

</div>



<p>This command will create a directory structure for your Scrapy project, including all the necessary files and boilerplate to get you started:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>\&gt;quotes\_project/  
    scrapy.cfg  
    quotes\_project/  
        \_\_init\_\_.py  
        items.py  
        middlewares.py  
        pipelines.py  
        settings.py  
        spiders/  
            \_\_init\_\_.py  
            quotes\_spider.py
</code></pre>

</div>



<h1>
  
  
  Here‚Äôs how they all work together:
</h1>

<ul>
<li>  scrapy.cfg: This is the Scrapy project configuration file. It contains settings and configurations for your Scrapy project.</li>
<li>  quotes_project(directory): This is the Python package for your Scrapy project.</li>
<li>  <a href="http://init.py/">init.py</a>: This is an empty Python file that makes the directory a Python package.</li>
<li>  <a href="http://items.py/">items.py</a>: This is where you define the structure of the items that will hold the scraped data. You create item classes with fields that correspond to the data you want to scrape.</li>
<li>  <a href="http://middlewares.py/">middlewares.py</a>: This file is used to define custom middleware components for your Scrapy project. Middleware can modify requests and responses during the scraping process.</li>
<li>  <a href="http://pipelines.py/">pipelines.py</a>: Here, you can define data processing pipelines to process and store the scraped data. You can implement actions like storing data in databases, exporting data to files, or performing additional processing.</li>
<li>  <a href="http://settings.py/">settings.py</a>: This is the main configuration file for your Scrapy project. You can set various project-specific settings, including user agent, concurrency, and more.</li>
<li>  spiders(directory): This directory contains your web-scraping spiders.</li>
<li>  <a href="http://init.py/">init.py</a>: This is an empty Python file that makes the directory a Python package.</li>
<li>  quotes_<a href="http://spider.py/">spider.py</a>: This is an example spider file where you define the spider to crawl and scrape data from websites. You create classes that inherit from <code>scrapy.Spider</code> and define how the spider navigates and extracts data from web pages. We'll create our first one together.</li>
</ul>

<p>This folder and file structure provide a clear organization for your Scrapy project, separating configuration, item definitions, spider code, and data processing logic.</p>

<p>Now that you have Scrapy installed and your project set up, let‚Äôs move on to defining a spider to scrape quotes from a website. Stay tuned for Step 3!</p>

<h1>
  
  
  Step 3: Creating the Item Class For Our Data Structure
</h1>

<p>In this step, we‚Äôll define the structure of the item that will hold the scraped data. Scrapy uses Items to structure and store the data you extract from websites.</p>

<p>Open the <code>[items.py](http://items.py/)</code> file in your Scrapy project directory. This file is automatically generated when you create your Scrapy project using the <code>scrapy startproject</code> command.</p>

<p>In <code>[items.py](http://items.py/)</code>, you'll notice the CLI has already created the QuotesScraperItem class, which inherits from scrapy.Item. Add the following code and remove the pass statement to define the structure of the <code>QuotesScraperItem</code>:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>import scrapy  
class QuotesScraperItem(scrapy.Item):  
    text = scrapy.Field()  
    author = scrapy.Field()
</code></pre>

</div>



<p>This code snippet defines an item class with two fields: <code>title</code> and <code>author</code>. You could also grab the <code>tags</code> data, but that's beyond the scope of this tutorial. These fields correspond to the data we'll scrape from the website.</p>

<p>With the item structure defined, we‚Äôre ready to move on to creating the spider, which is the class responsible for the extract portion of our ETL pipeline. I‚Äôve done the work of inspecting the target website and finding the selectors for the data we‚Äôll be scraping, too.</p>

<h1>
  
  
  Step 4: Defining a Scrapy Spider to Scrape Quotes
</h1>

<p>A Scrapy spider is a class that contains the rules for how to navigate and extract data from a website. Let‚Äôs get started:</p>

<ol>
<li> Create a new file named <code>quotes_spider.py</code> inside the <code>quotes_project/quotes_project/spiders</code> directory. This is where we'll define our spider.</li>
<li> Edit <code>quotes_spider.py</code> and add the code below
</li>
</ol>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>import scrapy  
from quotes\_project.items import QuotesScraperItem #made in the previous step  
class QuotesSpider(scrapy.Spider):  
            name = "quotes"  
            start\_urls = \[  
                'http://quotes.toscrape.com/page/1/',  
            \]  
            def parse(self, response):  
                for quote in response.css('div.quote'):  
                    item = QuotesScraperItem()  
                    item\['text'\] = ''.join(quote.css('span.text::text').get())  
                    item\['author'\] = ''.join(quote.css('span small::text').get())  
                    yield item  
                next\_page = response.css('li.next a::attr(href)').get()  
                if next\_page is not None:  
                    yield response.follow(next\_page, self.parse)
</code></pre>

</div>



<p>This code defines a Scrapy spider named ‚Äúquotes‚Äù that starts at the specified URL and scrapes quotes and their authors. It also continues to do so for each page until there are no more pages left. The ‚Äú‚Äù.join() string function is necessary because the ‚Äòtext‚Äô field will return a list of strings rather than one string, and this combines it for us easily.</p>

<ol>
<li> To run the spider and see it in action, use the following command in your project‚Äôs root directory:
</li>
</ol>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>scrapy crawl quotes
</code></pre>

</div>



<p>Now, your spider will start scraping quotes from the website. But you have nowhere to put it‚Ä¶ Fear not, that comes next!</p>

<h1>
  
  
  Step 5: Saving Scraped Data to a SQLite Database
</h1>

<p>Now that you‚Äôve successfully scraped data from the web, it‚Äôs time to store that data in a database for future use and analysis. In this step, we‚Äôll set up an SQLite database as the destination for the data in our pipeline automatically when we run the pipeline.</p>

<p>In Scrapy, pipelines are responsible for processing scraped data. We‚Äôll create a custom pipeline to insert our scraped items into the SQLite database we just created.</p>

<ol>
<li> In your Scrapy project folder, navigate to the <code>quotes_scraper</code> directory (or your project name) and open the <code>[pipelines.py](http://pipelines.py/)</code> file.</li>
<li> Define the following pipeline class at the end of the file.
</li>
</ol>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>import sqlite3  
            from itemadapter import ItemAdapter              
            class QuotesToSQLitePipeline:  
                def \_\_init\_\_(self):  
                    self.conn = sqlite3.connect('quotes.db')  
                    self.cursor = self.conn.cursor()  
                def process\_item(self, item, spider):  
                    adapter = ItemAdapter(item)  
                    self.cursor.execute('''  
                    CREATE TABLE IF NOT EXISTS quotes (  
                        id INTEGER PRIMARY KEY AUTOINCREMENT,  
                        text TEXT,  
                        author TEXT  
                    )  
                ''')  
                    self.cursor.execute('''  
                        INSERT INTO quotes (text, author)  
                        VALUES (?, ?)  
                    ''', (adapter\['text'\], adapter\['author'\]))  
                    self.conn.commit()  
                    return item  
                def close\_spider(self, spider):  
                    self.conn.close()
</code></pre>

</div>



<p>This pipeline class creates (if it doesn‚Äôt yet exist) or connects to the SQLite database, inserts scraped data into the <code>quotes</code> table and closes the connection when the spider is done. The ItemAdapter allows us to data within an Item without having to import that Item itself (less tightly coupled, yay!).</p>

<h1>
  
  
  Step 3: Enabling the Pipeline
</h1>

<p>To enable your custom pipeline, add it to the <code>[settings.py](http://settings.py/)</code> file in your Scrapy project folder:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>\# settings.py  
\# ...  
        # Configure item pipelines  
        ITEM\_PIPELINES = {  
           'quotes\_scraper.pipelines.QuotesToSQLitePipeline': 300,  # Adjust the priority if needed  
        }  
        # ...
</code></pre>

</div>



<p>In this example, I‚Äôve set the pipeline‚Äôs priority to 300, but you can adjust it as necessary.</p>

<h1>
  
  
  Step 4: Running Your Spider and Saving Data
</h1>

<p>Now that everything is set up, you can run your spider again and see how Scrapy saves scraped data to the SQLite database:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>scrapy crawl quotes
</code></pre>

</div>



<p>Scrapy will execute your spider and use the custom pipeline to save the scraped quotes and authors to the <code>quotes.db</code> database, which will now be in your project folder.</p>

<p>That‚Äôs it for Step 4! You‚Äôve successfully set up an SQLite database and configured a pipeline to save your scraped data. In the next steps, we‚Äôll cover how to retrieve and analyze the data.</p>

<h1>
  
  
  Step 5: Retrieving Data from the SQLite Database
</h1>

<p>You‚Äôve successfully scraped and saved data to an SQLite database using Scrapy. In this step, we‚Äôll explore how to retrieve that data from the database and perform basic queries.</p>

<h1>
  
  
  Step 1: Connecting to the Database
</h1>

<p>To retrieve data from the SQLite database, we need to establish a connection and create a cursor object, then perform a select, and finally print what‚Äôs returned from the cursor. Create a file called retrieve_<a href="http://quotes.py/">quotes.py</a> in the project‚Äôs root folder and copy in the following:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>import sqlite3  

def retrieve\_quotes():  
  # Create a context manager for the SQLite connection  
  with sqlite3.connect('quotes.db') as conn:  
      # Create a cursor within the context manager  
      cursor = conn.cursor   
      # Execute an SQL query to retrieve all quotes  
      cursor.execute('SELECT \* FROM quotes')                      
      # Fetch all the results from the cursor; fetchall() function returns a list.  
      quotes = cursor.fetchall(  
      # Display the retrieved data  
      for quote in quotes:  
          print(quote)  
if \_\_name\_\_ == "\_\_main\_\_":  
  retrieve\_quotes()
</code></pre>

</div>



<h1>
  
  
  Step 2: Running the Script
</h1>

<p>To run the script and retrieve data from the database, use the following command or run it in your IDE:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>python retrieve\_data.py
</code></pre>

</div>



<p>The script will connect to the database, retrieve the data, and display it in the terminal.</p>

<p>That‚Äôs it for Step 5! You‚Äôve learned how to retrieve data from the SQLite database where you saved your scraped quotes and authors. In the next steps, we‚Äôll dive deeper into data analysis and visualization.</p>

<h1>
  
  
  Step 6: (For Fun) Data Analysis and Visualization
</h1>

<p>In this optional step, you can explore your scraped data further by performing data analysis and visualization. We‚Äôll use Python libraries like Pandas and Matplotlib to accomplish this. This is barely enough data to bother with, but I thought it‚Äôd be nice to show this step to complete the most minimal of MVPs for a complete Scrapy pipeline use case!</p>

<h1>
  
  
  Step 1: Importing Required Libraries
</h1>

<p>First, make sure you have the necessary Python libraries installed. You can install them using pip:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>pip install pandas matplotlib
</code></pre>

</div>



<h1>
  
  
  Step 2: Analyzing Data
</h1>

<p>Now, let‚Äôs perform some basic analysis of the scraped data. We‚Äôll use Pandas to load the data from the SQLite database into a DataFrame and then calculate some statistics. In the project‚Äôs root folder, create a Python script called <a href="http://analysis.py/">data_analysis.py</a> and add the following code. This will print the total number of records using pandas.<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>import pandas as pd  
import sqlite3  
\# Connect to the SQLite database  
conn = sqlite3.connect('quotes.db')  
\# Load data into a Pandas DataFrame  
df = pd.read\_sql\_query('SELECT \* FROM quotes', conn)  
\# Calculate the number of quotes  
total\_quotes = len(df)  
\# Display the total number of quotes  
print(f'Total Number of Quotes: {total\_quotes}')
</code></pre>

</div>



<h1>
  
  
  Step 3: Visualizing Data
</h1>

<p>Next, let‚Äôs create a simple bar chart to visualize the distribution of quotes by author. We‚Äôll use Matplotlib for this purpose. Add the import to the top of <a href="http://analysis.py/">data_analysis.py</a> and copy-paste everything from the body of the code below to the end of <a href="http://analysis.py/">data_analysis.py</a>:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>import matplotlib.pyplot as plt  
\# Group quotes by author and count the occurrences  
author\_counts = df\['author'\].value\_counts()  
\# Create a bar chart  
plt.figure(figsize=(12, 6))  
author\_counts.plot(kind='bar')  
plt.title('Quotes by Author')  
plt.xlabel('Author')  
plt.ylabel('Number of Quotes')  
plt.xticks(rotation=45)  
plt.tight\_layout()  
plt.show()
</code></pre>

</div>



<h1>
  
  
  Step 4: Running the Script
</h1>

<p>Run the script using the following command:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>python data\_analysis.py
</code></pre>

</div>



<p>After running the script, you‚Äôll see the total number of quotes displayed in the console, and a bar chart showing the distribution of quotes by author will pop up.</p>

<p>That concludes Step 6, where you have the option to perform data analysis and visualization on your scraped data.</p>

<h1>
  
  
  Recap and Next Steps
</h1>

<p>Congratulations! You‚Äôve completed my tutorial on web scraping and data extraction process using Scrapy. At this point, you will have:</p>

<ol>
<li> Created a virtual environment for your Python project to manage dependencies.</li>
<li> Set up a Scrapy project and defined a web scraping spider.</li>
<li> Extracted quotes and authors from a website using Scrapy.</li>
<li> Created an SQLite database and stored the scraped data.</li>
<li> Implemented a Scrapy pipeline to automate data storage.</li>
<li> Performed data analysis and visualization on the scraped data.</li>
</ol>

<h1>
  
  
  Next Steps
</h1>

<p>Now that you have a solid foundation in web scraping with Scrapy, you can explore more advanced topics and real-world applications. In the coming weeks, I will be writing and linking in tutorials that build off this simple pipeline. They‚Äôll cover such topics as:</p>

<ul>
<li>  Scraping data from multiple websites and combining it.</li>
<li>  Handling different data formats and structures.</li>
<li>  Scheduling web scraping tasks to run at specific intervals.</li>
<li>  Implementing user authentication for scraping behind login pages.</li>
<li>  Scaling up your scraping efforts with distributed systems.</li>
</ul>

<p>Feel free to apply the knowledge and techniques you‚Äôve learned here to gather data for your projects, research, or data analysis tasks. If you make something cool out of a Scrapy pipeline after reading this, please let me know! I‚Äôd love to see it!</p>

<p>Scrape responsibly, and have fun!</p>

 </details> 
 <hr /> 

 #### - [How to animate Fire TV splash screens with React Native & Lottie](https://dev.to/amazonappdev/how-to-animate-fire-tv-splash-screens-with-react-native-lottie-32ca) 
 <details><summary>Article</summary> <blockquote>
<p><em><strong>Author's Note:</strong> In my <a href="https://dev.to/amazonappdev/develop-animated-splash-screens-on-fire-tv-with-lottie-5emp">previous Lottie article</a>, we covered how to create animated custom splash screens on Fire TV using Kotlin for Android and <a href="https://github.com/airbnb/lottie-android">Lottie</a>. For this article, we'll focus on those of you building Android apps using React Native. Since Lottie supports React Native, we'll cover the essentials to get you up and running quickly. Let's dive in!</em></p>
</blockquote>

<p><strong>Quick recap for Lottie</strong><br>
<a href="https://airbnb.io/lottie/#/">Lottie</a> is an open-source animation library created by Airbnb to render complex animations on mobile devices in real time. It's an excellent choice for creating interactive animations for both Android and iOS. Since Fire OS is based on the Android Open source Project <a href="https://source.android.com/">AOSP</a> we can use Lottie to render complex animated splash screens on Fire TV apps with React Native.</p>

<p>Let's now walk through how to create an animated splash screen for a React Native app running on Fire TV!</p>
<h3>
  
  
  <strong>Prerequisite:</strong> Create a React Native project
</h3>

<p>You can use your own existing React Native project or create a new one using the command line:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight shell"><code>npx react-native@latest init SplashScreenApp
</code></pre>

</div>



<blockquote>
<p>üí° <strong>Note:</strong> If you're new to React Native and want to learn how to setup your development environment and command line tools, head over to <a href="https://reactnative.dev/docs/environment-setup">the React Native official docs</a> to get up and running.</p>
</blockquote>

<h3>
  
  
  <strong>Step 1:</strong> Prepare the Lottie animation
</h3>

<p>To integrate Lottie animations into our TV splash screen, we need to first build an animation to match our app's branding and theme. For this example, we'll use the <a href="https://lottiefiles.com/animations/tv-watching-g5Lr55wxgp">TV Watching</a> animation by <a href="https://lottiefiles.com/wovencontent">Alan Murphy</a> available as a sample from <a href="https://lottiefiles.com/">LottieFiles</a>. </p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--zQ-B9c60--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/pvitz7foralsqe9gmbwe.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--zQ-B9c60--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/pvitz7foralsqe9gmbwe.png" alt="Lottie animation from LottieFiles" width="800" height="369"></a></p>

<h3>
  
  
  <strong>Step 2:</strong> Place the Lottie animation into the React Native app project
</h3>

<p>Create a directory <code>assets</code> in your React Native project and download the "<a href="https://lottiefiles.com/animations/tv-watching-g5Lr55wxgp">TV Watching Animation</a>" in Lottie JSON format from the step above. Place the JSON file in the new <code>assets</code> folder and make sure to rename the JSON file <code>splashscreen.json</code>.</p>

<h3>
  
  
  <strong>Step 3:</strong> Include Lottie as a package dependency
</h3>

<p>From the command line, inside the root directory of your React Native app execute the command to add Lottie as a dependency:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight shell"><code>yarn add lottie-react-native
</code></pre>

</div>



<h3>
  
  
  <strong>Step 4:</strong> Define the main components for your splash screen
</h3>

<p>For this sample app, we'll declare all required components inside <code>App.tsx</code>. </p>

<p>The 3 components are:</p>

<ol>
<li>
<code>App</code>, which represents the main app component</li>
<li>
<code>SplashScreen</code> for initiating the splash screen</li>
<li>
<code>MainContent</code> for the view that is shown after the splash screen disappears</li>
</ol>

<p>Let's open the file <code>App.tsx</code> and remove all the content generated by the React Native command line.</p>

<p>Then at the top of the file, add the required imports for the components we need:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight typescript"><code><span class="k">import</span> <span class="nx">React</span><span class="p">,</span> <span class="p">{</span> <span class="nx">useState</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">'</span><span class="s1">react</span><span class="dl">'</span><span class="p">;</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">Text</span><span class="p">,</span> <span class="nx">View</span><span class="p">,</span> <span class="nx">StyleSheet</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">'</span><span class="s1">react-native</span><span class="dl">'</span><span class="p">;</span>
<span class="k">import</span> <span class="nx">LottieView</span> <span class="k">from</span> <span class="dl">'</span><span class="s1">lottie-react-native</span><span class="dl">'</span><span class="p">;</span>
</code></pre>

</div>



<p>Next up you will declare the 3 components:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight typescript"><code><span class="kd">const</span> <span class="nx">SplashScreen</span> <span class="o">=</span> <span class="p">({</span> <span class="nx">onAnimationFinish</span> <span class="p">})</span> <span class="o">=&gt;</span> <span class="p">(</span>

<span class="p">);</span>

<span class="kd">const</span> <span class="nx">MainContent</span> <span class="o">=</span> <span class="p">()</span> <span class="o">=&gt;</span> <span class="p">(</span>

<span class="p">);</span>

<span class="kd">const</span> <span class="nx">App</span> <span class="o">=</span> <span class="p">()</span> <span class="o">=&gt;</span> <span class="p">{</span>

<span class="p">};</span>
</code></pre>

</div>



<p>and in addition set the styles we will apply for each of them:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight typescript"><code>
<span class="kd">const</span> <span class="nx">styles</span> <span class="o">=</span> <span class="nx">StyleSheet</span><span class="p">.</span><span class="nx">create</span><span class="p">({</span>
  <span class="na">container</span><span class="p">:</span> <span class="p">{</span>
    <span class="na">flex</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="na">justifyContent</span><span class="p">:</span> <span class="dl">'</span><span class="s1">center</span><span class="dl">'</span><span class="p">,</span>
    <span class="na">alignItems</span><span class="p">:</span> <span class="dl">'</span><span class="s1">center</span><span class="dl">'</span><span class="p">,</span>
  <span class="p">},</span>
  <span class="na">splashContainer</span><span class="p">:</span> <span class="p">{</span>
    <span class="na">width</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
    <span class="na">height</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
  <span class="p">},</span>
  <span class="na">splash</span><span class="p">:</span> <span class="p">{</span>
    <span class="na">flex</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="p">},</span>
  <span class="na">mainContainer</span><span class="p">:</span> <span class="p">{</span>
    <span class="na">marginTop</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
  <span class="p">},</span>
  <span class="na">text</span><span class="p">:</span> <span class="p">{</span>
    <span class="na">fontWeight</span><span class="p">:</span> <span class="dl">'</span><span class="s1">bold</span><span class="dl">'</span><span class="p">,</span>
  <span class="p">},</span>
<span class="p">});</span>
</code></pre>

</div>



<p>Remember to export the <code>App</code> component at the end of the file:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight typescript"><code><span class="k">export</span> <span class="k">default</span> <span class="nx">App</span><span class="p">;</span>
</code></pre>

</div>



<h3>
  
  
  <strong>Step 5:</strong> Implement the SplashScreen component
</h3>

<p>Implement the SplashScreen component by declaring its components. For this we will have a View containing a <code>LottieView</code> with the previous json animation file assigned to the <code>source</code> property of the <code>LottieView</code>.</p>

<p>We also define the <code>autoPlay</code> property in the <code>LottieView</code> in order to automatically start the animation and set the <code>loop</code> property as false so that we only show the animation one time:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight typescript"><code><span class="kd">const</span> <span class="nx">SplashScreen</span> <span class="o">=</span> <span class="p">({</span> <span class="nx">onAnimationFinish</span> <span class="p">})</span> <span class="o">=&gt;</span> <span class="p">(</span>
<span class="o">&lt;</span><span class="nx">View</span> <span class="nx">style</span><span class="o">=</span><span class="p">{</span><span class="nx">styles</span><span class="p">.</span><span class="nx">splashContainer</span><span class="p">}</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="nx">LottieView</span>
      <span class="nx">style</span><span class="o">=</span><span class="p">{</span><span class="nx">styles</span><span class="p">.</span><span class="nx">splash</span><span class="p">}</span>
      <span class="nx">source</span><span class="o">=</span><span class="p">{</span><span class="nx">require</span><span class="p">(</span><span class="dl">'</span><span class="s1">./assets/splashscreen.json</span><span class="dl">'</span><span class="p">)}</span>
      <span class="nx">autoPlay</span>
      <span class="nx">loop</span><span class="o">=</span><span class="p">{</span><span class="kc">false</span><span class="p">}</span>
      <span class="nx">onAnimationFinish</span><span class="o">=</span><span class="p">{</span><span class="nx">onAnimationFinish</span><span class="p">}</span>
    <span class="sr">/</span><span class="err">&gt;
</span>  <span class="o">&lt;</span><span class="sr">/View</span><span class="err">&gt;
</span>  <span class="p">);</span>
</code></pre>

</div>



<h3>
  
  
  <strong>Step 6:</strong> Implement the MainContent component
</h3>

<p>To start, this step will just show Text and then you choose when to display your app's home screen after the Splash Screen animation ends:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight typescript"><code><span class="kd">const</span> <span class="nx">MainContent</span> <span class="o">=</span> <span class="p">()</span> <span class="o">=&gt;</span> <span class="p">(</span>
  <span class="o">&lt;</span><span class="nx">View</span> <span class="nx">style</span><span class="o">=</span><span class="p">{</span><span class="nx">styles</span><span class="p">.</span><span class="nx">mainContainer</span><span class="p">}</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="nx">Text</span> <span class="nx">style</span><span class="o">=</span><span class="p">{</span><span class="nx">styles</span><span class="p">.</span><span class="nx">text</span><span class="p">}</span><span class="o">&gt;</span><span class="nx">Your</span> <span class="nx">Awesome</span> <span class="nx">App</span> <span class="nx">Home</span> <span class="nx">Screen</span><span class="o">!&lt;</span><span class="sr">/Text</span><span class="err">&gt;
</span>  <span class="o">&lt;</span><span class="sr">/View</span><span class="err">&gt;
</span><span class="p">);</span>
</code></pre>

</div>



<h3>
  
  
  <strong>Step 7:</strong> Implement the App component
</h3>

<p>The App component handles the logic to display and hide the Splash screen. </p>

<p>Declare a <code>loaded</code> state and set it to false. This allows the the animation to load prior to the Splash Screen being rendered on screen. Once the animation resource is ready, a callback will flip the <code>loaded</code> state to true to then make the <code>MainContent</code> component visible on the Fire TV:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight typescript"><code><span class="kd">const</span> <span class="nx">App</span> <span class="o">=</span> <span class="p">()</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="kd">const</span> <span class="p">[</span><span class="nx">loaded</span><span class="p">,</span> <span class="nx">setLoaded</span><span class="p">]</span> <span class="o">=</span> <span class="nx">useState</span><span class="p">(</span><span class="kc">false</span><span class="p">);</span>

  <span class="kd">const</span> <span class="nx">handleAnimationFinish</span> <span class="o">=</span> <span class="p">()</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="nx">setLoaded</span><span class="p">(</span><span class="kc">true</span><span class="p">);</span>
  <span class="p">};</span>

  <span class="k">return</span> <span class="p">(</span>
    <span class="o">&lt;</span><span class="nx">View</span> <span class="nx">style</span><span class="o">=</span><span class="p">{</span><span class="nx">styles</span><span class="p">.</span><span class="nx">container</span><span class="p">}</span><span class="o">&gt;</span>
      <span class="p">{</span><span class="nx">loaded</span> <span class="p">?</span> <span class="o">&lt;</span><span class="nx">MainContent</span> <span class="o">/&gt;</span> <span class="p">:</span> <span class="o">&lt;</span><span class="nx">SplashScreen</span> <span class="nx">onAnimationFinish</span><span class="o">=</span><span class="p">{</span><span class="nx">handleAnimationFinish</span><span class="p">}</span> <span class="sr">/&gt;</span><span class="err">}
</span>    <span class="o">&lt;</span><span class="sr">/View</span><span class="err">&gt;
</span>  <span class="p">);</span>
<span class="p">};</span>
</code></pre>

</div>



<h2>
  
  
  <strong>Wrap Up:</strong> Load up and test our animated splash screen!
</h2>

<p>Now we can test either by using <a href="https://developer.amazon.com/docs/fire-tv/connecting-adb-to-device.html">Fire TV via adb</a> or <a href="https://developer.android.com/training/tv/start/start#run-on-a-virtual-device">starting an Android TV emulator</a>. </p>

<p>Either approach will let us see the sample in action by running this command from the project directory:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>npm start
</code></pre>

</div>



<p>Remember to press <code>a</code> to run the Android version.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--JkM_SjJq--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/tyxq87fbnnwiylijvikd.gif" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--JkM_SjJq--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/tyxq87fbnnwiylijvikd.gif" alt="Animated Splash screen" width="600" height="714"></a></p>

<p>Using <a href="https://github.com/lottie-react-native/lottie-react-native">Lottie for React Native</a> is an easy way to enrich your app with beautiful animations and make your user happy.</p>

<p>Thanks for reading this guide and make sure to check out the sample project source code <a href="https://github.com/giolaq/splash-screen-tv-app-react-native">on Github</a>.</p>

<p><strong>Stay updated</strong> </p>

<p>For the latest Amazon Appstore developer news, product releases, tutorials, and more:</p>

<p>üì£ Follow <a href="https://twitter.com/AmazonAppDev">@AmazonAppDev</a> and <a href="https://twitter.com/i/lists/1580293569897984000/members">our team</a> on Twitter</p>

<p>üì∫ Subscribe to our <a href="https://www.youtube.com/amazonappstoredevelopers">Youtube channel</a></p>

<p>üìß Sign up for the <a href="https://m.amazonappservices.com/devto-newsletter-subscribe">Developer Newsletter</a></p>

<h3>
  
  
  About the author
</h3>

<p>Giovanni ("Gio") Laquidara is a Senior Dev Advocate at Amazon, where he works on supporting developers around the world using the Amazon Appstore across many devices. </p>

<p>Previously, Gio worked as a software engineer building mobile apps, real-time defence systems, and VR/AR experiences. For fun, Gio enjoys low level programming, IoT hacking, and command line apps ‚å®Ô∏è‚ú®.</p>

<p><em>You can connect with Gio on <a href="https://twitter.com/giolaq">Twitter</a>, <a href="https://www.linkedin.com/in/glaquidara/">Linkedin</a>, and <a href="https://giolaq.dev">giolaq.dev</a>.</em></p>

 </details> 
 <hr /> 
<!-- BLOG-POST-LIST:END -->
</table>
</details>


<!-- TODO
Change the 3stats boxes around, possibly two on top and one on bottom
Fix RSSfeed
Fix Spotify Playlists
Fix Socials [Portfolio, Discord, Linkedin]
In the future, add Public Repositories of Selected Projects
-->
