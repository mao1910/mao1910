<!-- VISITOR BADGE -->
<!-- https://github.com/hehuapei/visitor-badge -->

<img align="right" src="https://visitor-badge.laobi.icu/badge?page_id=mao1910.mao1910&left_color=%2379DAF9&right_color=%23FE6E96" />


<!-- TYPING SVG -->
<!-- https://github.com/DenverCoder1/readme-typing-svg -->

<h1 align="center">
    <img src="https://readme-typing-svg.herokuapp.com/?font=Righteous&size=35&center=true&vCenter=true&width=500&height=70&color=FE6E96&font=poppins&duration=5000&lines=Hi+There!+👋;+I'm+Mao!;" />
</h1>

<br/>

<!-- CODE/TERMINAL ABOUT ME -->
<h1 align="center">
<img src="./assets/terminal-5.gif" alt="Terminal" />
</h1>

<br/><br/><br/>


<!-- TECHNOLOGIES LOGOS -->
<!-- https://github.com/tandpfun/skill-icons -->

<h2 align="center">💻 Languages / Frameworks / Tools ⚒️</h2>
<div align="center">
    <img src="https://skillicons.dev/icons?i=javascript,typescript,angular,react,html,css,scss,bootstrap,cs,java,spring" />
    <img src="https://skillicons.dev/icons?i=flutter,firebase,supabase,mysql,git,github,gitlab,vscode,idea,maven,figma" />
</div>

<br/><br/><br/>


<!-- CONTRIBUTIONS SNAKE GAME -->
<!-- https://github.com/Platane/snk -->

<div align="center">
  <h2> My Contributionssss🐍 </h2>
  <br>
  <img alt="contributions-eating Snake" src="https://raw.githubusercontent.com/mao1910/mao1910/output/github-contribution-grid-snake.svg" />

  <!-- Four lines below suggested by Planate for Dark mode-->
  <picture>
  <source media="(prefers-color-scheme: dark)" srcset="github-snake-dark.svg" />
  <source media="(prefers-color-scheme: light)" srcset="github-snake.svg" />
  </picture>
  
  <br/><br/><br/>
</div>


<!-- GITHUB STATS -->
<!-- https://github.com/DenverCoder1/github-readme-streak-stats -->
<!-- https://github.com/anuraghazra/github-readme-stats -->
<!-- https://github-readme-stats-mao1910.vercel.app/ My own Vercel deployment-->

<h2 align="center"> Stats📝 </h2>
  <br>
<div align=center>
  <img width=429 src="https://github-readme-stats-mao1910.vercel.app/api?username=mao1910&count_private=true&show_icons=true&theme=dracula&rank_icon=github&hide=contribs&border_radius=10&border_color=79DAF9" alt="github stats"/>
  <img width=396 src="https://streak-stats.demolab.com/?user=mao1910&count_private=true&theme=dracula&currStreakNum=79DAF9&currStreakLabel=FE6E96&border_radius=10&border=79DAF9" alt="streak stats"/>
  <br/>
  <img src="https://github-readme-stats-mao1910.vercel.app/api/top-langs/?username=mao1910&layout=compact&theme=dracula&border_radius=10&size_weight=0.5&count_weight=0.5&border_color=79DAF9" alt="languages stats" />
</div>

<br/><br/><br/>


<!-- FOOTER -->
<!-- https://github.com/DenverCoder1/readme-typing-svg -->
<!-- https://readme-typing-svg.demolab.com/demo/ -->

<a href="https://git.io/typing-svg"><img src="https://readme-typing-svg.demolab.com?font=Poppins&pause=1000&color=FE6E96&width=535&lines=Thanks+for+dropping+by!;Feel+free+to+check+any+of+the+Socials+below+%F0%9F%91%87;Or+the+Joke+Of+The+Day+if+you're+down+for+a+giggle+%F0%9F%98%9D;Hope+to+see+you+again+%F0%9F%91%8A;Uh%3F+You're+still+here%3F;Well...+I'm+running+out+of+things+to+say...;Tell+you+what%2C+due+to+your+effort+and+perseverance%2C;I+shall+present+you+with+a+short+poem%3A;%22To+code%2C+or+not+to+code%2C+that+is+the+question%3A;Whether+'tis+nobler+in+the+IDE+to+debug;The+errors+and+issues+of+outrageous+software%2C;Or+to+take+up+the+keyboard+against+a+sea+of+bugs;And+by+coding%2C+end+them.%22;by+William+Shakespeare%2C+probably.+;Pretty+sure+that's+Hamlet's.;Alrighty%2C+this+has+been+fun.;But+I'll+restart+the+loop+now...+see+ya+soon!" alt="Typing SVG" /></a>


<!--  SOCIAL NETWORKS -->
<!-- https://github.com/alexandresanlim/Badges4-README.md-Profile -->

  <div> 
    <a href="https://www.deviantart.com/madeinkobaia/art/my-profile-is-under-construction-265626465" target="_blank"><img src="https://img.shields.io/badge/-LinkedIn-%230077B5?style=for-the-badge&logo=linkedin&logoColor=white" target="_blank"></a> <!-- ADD LINKEDIN PROFILE -->
    <a href = "https://www.nicepng.com/ourpic/u2q8o0t4t4r5o0r5_website-under-construction-png-graphic-transparent-website-under/"><img src="https://img.shields.io/badge/Portfolio-4285F4?style=for-the-badge&logo=Google-chrome&logoColor=white" target="_blank"></a> <!-- ADD PORTFOLIO WEBSITE -->
    <a href="https://discord.gg" target="_blank"><img src="https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white" target="_blank"></a> <!-- ADD DISCORD -->
    <a href = "mailto:mao1910dev@gmail.com"><img src="https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&logo=gmail&logoColor=white" target="_blank"></a>
  </div>


<!-- SPOTIFY PLAYING-->
<!-- https://github.com/novatorem/novatorem -->
<!-- https://spotify-now-playing-novatorem-git-main-mao1910.vercel.app/ My own Vercel deployment-->

[<img width=438px src="https://spotify-now-playing-git-main-mao1910.vercel.app//api/spotify/?border_color=FE6E96" alt="Mao Spotify Now Playing" />](https://open.spotify.com/user/31542et242zglhf42ydrtqgvuvde)


<!-- JOKE OF THE DAY -->
<!-- https://github.com/ABSphreak/readme-jokes -->
<!-- https://readme-jokes-git-master-mao1910.vercel.app/ My own Vercel deployment-->

<details>
<summary>I've got a Joke for you. Wanna hear it? 🙈</summary>

<br/>

 <tr>
 <td style="padding-top:4px"><img src = "https://readme-jokes-git-master-mao1910.vercel.app/api?&theme=dracula"></td>
 </tr>

</details>


<!-- RSS FEED -->
<!-- https://github.com/gautamkrishnar/blog-post-workflow -->

<details>
<summary>📕 &nbsp;RSS feed</summary>

<br/>

<!-- BLOG-POST-LIST:START -->
 #### - [If you have these three things, then you’re a user (and not a programmer)](https://dev.to/noriller/if-you-have-these-three-things-then-youre-a-user-and-not-a-programmer-45nk) 
 <details><summary>Article</summary> <p>2023/09/13 is the 256 day of the year or in other words, Programmer’s Day.</p>

<p>Take this chance to congratulate yourself, your colleagues, or a programmer friend.</p>

<blockquote>
<p><strong>Disclaimer</strong>: the author take no responsibility for the trip to the nearest dark place your programmer friend might be in.</p>
</blockquote>

<h2>
  
  
  Programmer or user?
</h2>

<p>In other years ([<a href="https://dev.to/noriller/if-you-have-these-three-things-you-can-be-a-programmer-too-v2-2e56">2022</a>, <a href="https://dev.to/noriller/if-you-have-this-three-things-you-can-be-a-programmer-too-28b8">2021</a>]) I’ve talked about things that make you a Programmer, but this time around I enumerated a few things that might show that you might not be a programmer, but (<em>suspense noises</em>) just a user.</p>

<p>Please note that everyone <em>is</em> a user of something, that is not a problem. But if you say you’re a programmer, but all you do is be a user, then you have a <code>TypeError: User is not of type Programmer</code>.</p>

<h2>
  
  
  1. You don't read error messages
</h2>

<p>I’ve worked a lot with digitally illiterate people of many levels and the one thing that stood out was how fast they would click an error message out of view.</p>

<p>Then there I went, did what they were doing, <strong>actually</strong> read the error message, and <em>magic!</em> solved the problem.</p>

<p>I saw “programmers” who do something similar, choosing to ignore any error messages, trying to run whatever they’re doing again, and, of course, failing.</p>

<p>Sometimes, all it takes is just reading the error messages. Other times, you read, google it, open the first link, and follow the instructions. <em>Presto!</em></p>

<p>Other times, you have to bang your two neurons a little harder, tracing the steps of the software to pinpoint the exact point of failure, and then facepalm yourself hard enough because it was something so simple.</p>

<p>Finally, a few times you read the error, then backtraced the problem and still you and ChatGPT have no idea how to solve it. So, you read the <strong><a href="https://stackoverflow.com/help/how-to-ask">How do I ask a good question?</a></strong> page from StackOverflow and wherever you usually do a good question so people can easily help you.</p>

<h2>
  
  
  2. You’re content with software
</h2>

<p>The more junior you are the more you probably think: “I can do it better”.</p>

<p>On the other side, the more senior you get, probably the more pissed you get with badly written websites and apps. You also probably appreciate good interactions a lot more.</p>

<p>This is if you’re a programmer, but when you’re a user… You just don’t care.</p>

<p>You might think you can just jump the hoops and loops and all those problems are nothing to get worked over. You’re complacent and only the very worst will make you complain or abandon it.</p>

<p>No matter who you are, we always end up drawing a line where from one point forward we are programmers but the rest we are users. The question is where did you draw yours?</p>

<p>The language? The framework? The meta framework? The libs? Just whatever you’re doing?</p>

<p>Some people might never have thought about actually influencing the things and tools they use every day. You might not have time or skill to actually fix something, but you see something you use and feel like it could be improved with something or you have the same problem over and over… have you ever tried opening an Issue in the project repo?</p>

<p>Maybe more people have the same problem or more people would enjoy the improvements of your idea, someone might jump into implementing that if only they see your Issue. But when you just don’t care enough, then nothing will happen.</p>

<h2>
  
  
  3. You’re superstitious (it’s how it was always done)
</h2>

<p>This one touches the other ones. The superstitious “programmer” (or user) is someone who does things because “it was how it was always done”.</p>

<p>I can argue that you don’t actually know what in the name of binary you’re doing, because you just copy and paste code, change things here and there, and hope it works. I’ve met that kind of “programmer” and nowadays one of its names is ChatGPT.</p>

<p>ChatGPT is a grand example because it doesn't know anything. It just saw enough it can just spill enough bullshit that sometimes it actually makes sense and works.</p>

<p>jQuery still works to this day, but is it the best way of doing things today? While jumping at the newest framework would definitely make you a programmer, it’s not something sustainable.</p>

<p>A programmer has to evaluate both ends and find the one that is the best choice for today and for the foreseeable future, always considering where they are coming from.</p>

<p>In a new project, this is easier, but in a legacy project this might be a rewrite or more likely, adding a new way of doing things where you don’t need to rewrite things already working, but gives you a better tool to migrate crucial parts and create new features.</p>




<p>Cover Photo by <a href="https://unsplash.com/@thisisengineering?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">ThisisEngineering RAEng</a> on <a href="https://unsplash.com/photos/iQqRM0XJvn8?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></p>

 </details> 
 <hr /> 

 #### - [Build an AI SMS Chatbot with Replicate, LLaMA 2, and LangChain](https://dev.to/twilio/build-an-ai-sms-chatbot-with-replicate-llama-2-and-langchain-3i72) 
 <details><summary>Article</summary> <p>Recently, Meta and Microsoft introduced the second generation of the LLaMA LLM (Large Language Model) to help developers and organizations to build generative AI-powered tools and experiences. Read on to learn how to build an AI SMS chatbot that answers questions like Ahsoka (from Star Wars) using LangChain templating, LLaMa 2, Replicate, and Twilio Programmable Messaging!<br>
<a href="https://res.cloudinary.com/practicaldev/image/fetch/s--YtzI9bA---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/c11hzs7x148okb1zjlj3.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--YtzI9bA---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/c11hzs7x148okb1zjlj3.png" alt="SMS example" width="800" height="197"></a><br>
Do you prefer learning via video more? Check out <a href="https://www.tiktok.com/@lizziepikachu/video/7278020285750889774">this TikTok summarizing this tutorial</a> in 1 minute!</p>
<h3>
  
  
  Prerequisites
</h3>

<ol>
<li>A Twilio account - <a href="https://www.twilio.com/try-twilio">sign up for a free Twilio account here</a>
</li>
<li>A Twilio phone number with SMS capabilities - <a href="https://support.twilio.com/hc/en-us/articles/223135247-How-to-Search-for-and-Buy-a-Twilio-Phone-Number-from-Console">learn how to buy a Twilio Phone Number here</a>
</li>
<li>Replicate account to host the LlaMA 2 model – <a href="https://replicate.com/signin?next=/">make a Replicate account here</a>
</li>
<li>Python installed - <a href="https://www.python.org/downloads/">download Python here</a>
</li>
<li>
<a href="https://ngrok.com/download">ngrok</a>, a handy utility to connect the development version of our Python application running on your machine to a public URL that Twilio can access.</li>
</ol>

<p>⚠️ <strong>ngrok is needed for the development version of the application because your computer is likely behind a router or firewall, so it isn’t directly reachable on the Internet. You can also choose to automate ngrok as shown in this article.</strong></p>
<h3>
  
  
  Replicate
</h3>

<p>Replicate offers a cloud API and tools so you can more easily run machine learning models, abstracting away some lower-level machine learning concepts and handling infrastructure so you can focus more on your own applications. You can run open-source models that others have published, or package and publish your own, either publicly or privately.</p>
<h3>
  
  
  Configuration
</h3>

<p>Since you will be installing some Python packages for this project, you will need to make a new project directory and a <a href="https://docs.python.org/3/tutorial/venv.html">virtual environment</a>.</p>

<p>If you're using a Unix or macOS system, open a terminal and enter the following commands:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight shell"><code><span class="nb">mkdir </span>replicate-llama-ai-sms-chatbot  
<span class="nb">cd </span>replicate-llama-ai-sms-chatbot  
python3 <span class="nt">-m</span> venv venv 
<span class="nb">source </span>venv/bin/activate 
pip <span class="nb">install </span>langchain replicate flask twilio
</code></pre>

</div>



<p>If you're following this tutorial on Windows, enter the following commands in a command prompt window:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight shell"><code><span class="nb">mkdir </span>replicate-llama-ai-sms-chatbot  
<span class="nb">cd </span>replicate-llama-ai-sms-chatbot   
python <span class="nt">-m</span> venv venv 
venv<span class="se">\S</span>cripts<span class="se">\a</span>ctivate 
pip <span class="nb">install </span>langchain replicate flask twilio
</code></pre>

</div>



<p><a href="https://replicate.com/account/api-tokens">Grab your default Replicate API Token or create a new one here</a>.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--GbpFlrL6--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/aat6zydxbdnq8sohcc4x.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--GbpFlrL6--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/aat6zydxbdnq8sohcc4x.png" alt="Replicate console" width="800" height="226"></a><br>
On the command line run<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight shell"><code><span class="nb">export </span><span class="nv">REPLICATE_API_TOKEN</span><span class="o">={</span>replace with your api token<span class="o">}</span>
</code></pre>

</div>



<p>Now it's time to write some code!</p>

<h3>
  
  
  Code
</h3>

<p>Make a file called <em>app.py</em> and place the following import statements at the top.<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="kn">from</span> <span class="nn">flask</span> <span class="kn">import</span> <span class="n">Flask</span><span class="p">,</span> <span class="n">request</span>
<span class="kn">from</span> <span class="nn">langchain</span> <span class="kn">import</span> <span class="n">LLMChain</span><span class="p">,</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain.llms</span> <span class="kn">import</span> <span class="n">Replicate</span>
<span class="kn">from</span> <span class="nn">langchain.memory</span> <span class="kn">import</span> <span class="n">ConversationBufferWindowMemory</span>
<span class="kn">from</span> <span class="nn">twilio.twiml.messaging_response</span> <span class="kn">import</span> <span class="n">MessagingResponse</span>
</code></pre>

</div>



<p>Though LLaMA 2 is tuned for chat, templates are still helpful so the LLM knows what behavior is expected of it. This starting prompt is similar to ChatGPT so it should behave similarly.<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="n">template</span> <span class="o">=</span> <span class="s">"""Assistant is a large language model.

Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.

Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.

Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist. 

I want you to act as Ahsoka giving advice and answering questions. You will reply with what she would say.
SMS: {sms_input}
Assistant:"""</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span><span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s">"sms_input"</span><span class="p">],</span> <span class="n">template</span><span class="o">=</span><span class="n">template</span><span class="p">)</span>
</code></pre>

</div>



<p>Next, make a LLM Chain, one of the core components of LangChain. This allows us to chain together prompts and make a prompt history. The model is formatted as the model name followed by the version–in this case, the model is LlaMA 2, a 13-billion parameter language model from Meta fine-tuned for chat completions. <code>max_length</code> is 4096, the maximum number of tokens (called the <em>context window</em>) the LLM can accept as input when generating responses.<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="n">sms_chain</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span>
    <span class="n">llm</span> <span class="o">=</span> <span class="n">Replicate</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s">"a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5"</span><span class="p">),</span> 
    <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
    <span class="n">memory</span><span class="o">=</span><span class="n">ConversationBufferWindowMemory</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">llm_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s">"max_length"</span><span class="p">:</span> <span class="mi">4096</span><span class="p">}</span>
<span class="p">)</span>
</code></pre>

</div>



<p>Finally, make a Flask app to accept inbound text messages, pass that to the LLM Chain, and return the output as an outbound text message with Twilio Programmable Messaging.<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="n">app</span> <span class="o">=</span> <span class="n">Flask</span><span class="p">(</span><span class="n">__name__</span><span class="p">)</span>


<span class="o">@</span><span class="n">app</span><span class="p">.</span><span class="n">route</span><span class="p">(</span><span class="s">"/sms"</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="s">'GET'</span><span class="p">,</span> <span class="s">'POST'</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">sms</span><span class="p">():</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="n">MessagingResponse</span><span class="p">()</span>
    <span class="n">inb_msg</span> <span class="o">=</span> <span class="n">request</span><span class="p">.</span><span class="n">form</span><span class="p">[</span><span class="s">'Body'</span><span class="p">].</span><span class="n">lower</span><span class="p">().</span><span class="n">strip</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">sms_chain</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sms_input</span><span class="o">=</span><span class="n">inb_msg</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
    <span class="n">resp</span><span class="p">.</span><span class="n">message</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">resp</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">app</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">debug</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre>

</div>



<p>On the command line, run <code>python app.py</code> to start the Flask app.</p>

<h3>
  
  
  Configure a Twilio Number for the SMS Chatbot
</h3>

<p>Now, your Flask app will need to be visible from the web so Twilio can send requests to it. ngrok lets you do this. With ngrok installed, run <code>ngrok http 5000</code> in a new terminal tab in the directory your code is in.</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--nsM06Uhj--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2kgwrxcxdfgo9tkrp42o.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--nsM06Uhj--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2kgwrxcxdfgo9tkrp42o.png" alt="ngrok terminal tab" width="800" height="334"></a><br>
You should see the screen above. Grab that ngrok <strong>Forwarding URL</strong> to configure your Twilio number: select your Twilio number under <strong>Active Numbers</strong> in your <a href="https://www.twilio.com/console/phone-numbers/incoming">Twilio console</a>, scroll to the <strong>Messaging</strong> section, and then modify the phone number’s routing by pasting the ngrok URL with the <em>/sms</em> path in the textbox corresponding to when <strong>A Message Comes In</strong> as shown below:</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--YFXGG5Ul--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/s6lgkzfqevilybi3al5s.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--YFXGG5Ul--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/s6lgkzfqevilybi3al5s.png" alt="configure phone number" width="800" height="335"></a><br>
Click <strong>Save</strong> and now your Twilio phone number is configured so that it maps to your web application server running locally on your computer and your application can run. Text your Twilio number a question relating to the text file and get an answer from that file over SMS!</p>

<p><a href="https://res.cloudinary.com/practicaldev/image/fetch/s--NqUl4lle--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/3xi0flyo9fdgq4okmwgd.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--NqUl4lle--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/3xi0flyo9fdgq4okmwgd.png" alt="SMS example" width="800" height="226"></a><br>
You can view the <a href="https://github.com/elizabethsiegle/replicate-llama2-sms-chatbot">complete code on GitHub here</a>.</p>

<h3>
  
  
  What's Next for Twilio, LangChain, Replicate, and LLaMA 2?
</h3>

<p>There is so much fun for developers to have around building with LLMs! You can modify existing LangChain and LLM projects to use LLaMA 2 instead of GPT, build a web interface using <a href="https://streamlit.io/">Streamlit</a> instead of SMS, fine-tune LLaMA 2 with your own data, and more! I can't wait to see what you build–let me know online what you're working on!</p>

<ul>
<li>Twitter: <a href="https://twitter.com/lizziepika">@lizziepika</a>
</li>
<li>GitHub: <a href="https://github.com/elizabethsiegle">elizabethsiegle</a>
</li>
<li>Email: <a href="mailto:lsiegle@twilio.com">lsiegle@twilio.com</a>
</li>
</ul>

 </details> 
 <hr /> 

 #### - [Unlocking the Potential of Large Language Models (LLMs) for Business Insights](https://dev.to/devnenyasha/unlocking-the-potential-of-large-language-models-llms-for-business-insights-23n9) 
 <details><summary>Article</summary> <p>In the domain of computerized reasoning and normal language handling, Large Language Models (LLMs) have arisen as integral assets that are changing the manner in which organizations work. These models, like GPT-3, are not just about creating human-like text; they are tied in with opening the capability of information and language for significant bits of knowledge. In this article, we will investigate how LLMs can change the universe of business by giving important bits of knowledge and driving informed navigation.</p>

<p>The Ascent of LLMs</p>

<p>Large Language Models like GPT-3 are the finish of long stretches of exploration and development in AI. They are gigantic brain networks prepared on huge measures of text information from the web. This preparation empowers them to grasp settings, produce intelligent text, and play out an extensive variety of language-related errands with striking capability.</p>

<p>From Text Age to Knowledge Age</p>

<p>While text age is one of the most apparent utilizations of LLMs, their true capacity goes a long way past exploratory writing or chatbots. Organizations are tackling the force of LLMs to acquire profound experiences from printed information sources:</p>

<ol>
<li>Information Examination and Rundown:</li>
</ol>

<p>LLMs can filter through piles of unstructured information, extricating key data and summing up it in a conceivable organization. This smoothes out information examination processes, making it more straightforward for organizations to pursue information-driven choices.</p>

<ol>
<li>Statistical surveying and Serious Examination:</li>
</ol>

<p>By breaking down news stories, web-based entertainment discussions, and industry reports, LLMs can give continuous bits of knowledge into market patterns and contender techniques. This permits organizations to adjust rapidly to changing economic situations.</p>

<ol>
<li>Client Criticism and Opinion Investigation:</li>
</ol>

<p>LLMs succeed at opinion investigation. They can evaluate client audits and virtual entertainment presents to measure public feeling about items and administrations, assisting organizations with recognizing regions for development.</p>

<ol>
<li>Mechanized Announcing:</li>
</ol>

<p>Organizations can mechanize the formation of reports, synopses, and documentation utilizing LLMs. This lessens manual exertion and guarantees consistency in revealing.</p>

<p>Contextual analyses in real life</p>

<p>We should take a gander at a couple of genuine instances of how organizations are utilizing LLMs for bits of knowledge:</p>

<ol>
<li>Medical services Diagnostics:</li>
</ol>

<p>In the clinical field, LLMs are utilized to dissect patient records and clinical writing, assisting specialists with making precise determinations and treatment proposals.</p>

<ol>
<li>Monetary Administrations:</li>
</ol>

<p>Banks and monetary establishments use LLMs to break down news stories and monetary reports for venture experiences, risk appraisal, and misrepresentation discovery.</p>

<ol>
<li>Web-based business Personalization:</li>
</ol>

<p>Online retailers use LLMs to examine client perusing conduct and prescribe items custom-made to individual inclinations, supporting deals, and consumer loyalty.</p>

<p>Difficulties and Contemplations</p>

<p>While LLMs offer colossal potential, they additionally accompany difficulties and moral contemplations. Predisposition in preparing information, mindful computer-based intelligence improvement, and information security are a portion of the issues that should be tended to while sending LLMs in business settings.</p>

<p>Future Patterns</p>

<p>As LLM innovation keeps on developing, we can expect significantly further developed applications in regions like robotized content age, remote helpers, and redid instructive materials.</p>

<p>Conclusion</p>

<p>Huge Language Models have the ability to change how organizations work by opening significant bits of knowledge from text-based information sources. From information examination to statistical surveying and then some, LLMs are becoming important instruments for informed navigation. Notwithstanding, organizations should proceed cautiously, guaranteeing moral use and capable advancement as they outfit the capability of these strong simulated intelligence models. As we push ahead, LLMs will keep on assuming a significant part in forming the fate of business knowledge.</p>

 </details> 
 <hr /> 

 #### - [Next.js: How <Suspense /> and Components Streaming works?](https://dev.to/charnog/nextjs-how-and-components-streaming-works-30ao) 
 <details><summary>Article</summary> <blockquote>
<p>There is a distinct difference between ‘suspense’ and ‘surprise’, and yet many pictures continually confuse the two. I’ll explain what I mean. We are now having a very innocent little chat. Let’s suppose that there is a bomb underneath this table between us. Nothing happens, and then all of a sudden, ‘Boom!’ There is an explosion. The public is surprised, but prior to this surprise, it has seen an absolutely ordinary scene, of no special consequence.</p>

<p>Alfred Hitchcock</p>
</blockquote>

<p>In this article, we'll dive into the specifics of the <code>&lt;Suspense /&gt;</code> tag as it relates to Next.js and Server Side Rendering (SSR) feature. We'll delve deeper to see what happens at the HTTP protocol level when you wrap your components with the  tag. Let's begin.</p>

<h3>
  
  
  Streaming, what is it?
</h3>

<p>Before we dive into "Components Streaming" it's essential to understand what HTTP streaming is in and of itself. When your User Agent (for example, a browser or a <code>curl</code> command) sends an HTTP request to a server, the server replies with something like:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>HTTP/1.1 200 OK␍␊
Date: Mon, 27 Jul 2009 12:28:53 GMT␍␊
Content-Length: 12␍␊
Content-Type: text/plain␍␊
␍␊
Hello World!
</code></pre>

</div>



<p>I've added the ␍␊ to HTTP response texts because it carries a special meaning in HTTP.</p>

<p>The first line, <code>HTTP/1.1 200 OK</code>, tells us that everything is fine and the server has responded with a 200 OK code. Following this, we have three lines that are known as headers. In our example, these three headers are <code>Date</code>, <code>Content-Length</code>, and <code>Content-Type</code>. We can think of them as key-value pairs, where the keys and values are delimited by a <code>:</code> sign.</p>

<p>Following the headers, there's an empty line, serving as a delimiter between the header and the body sections. After this line, we encounter the content itself. Given the prior information from the headers, our browser understands two things:</p>

<ol>
<li>It needs to download 12 bytes of content (the string <code>Hello World!</code> comprises just 12 characters).</li>
<li>Once downloaded, it can display this content or provide it to the callback of a fetch request.</li>
</ol>

<p>In other words, we can deduce that the end of the response body will occur once we've read 12 bytes following a new line.</p>

<p>Now, what happens if we omit the <code>Content-Length</code> header from our server response? When the <code>Content-Length</code> header is absent, many HTTP servers will implicitly add a <code>Transfer-Encoding: chunked</code> header. This type of response can be interpreted as, "Hi, I'm the server, and I'm not sure how much content there will be, so I'll send the data in chunks." So a response will look like:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>HTTP/1.1 200 OK␍␊
Date: Mon, 27 Jul 2009 12:28:53 GMT␍␊
Transfer-Encoding: chunked␍␊
Content-Type: text/plain␍␊
␍␊
5␍␊
Hello␍␊
</code></pre>

</div>



<p>At this point, we haven't received the entire message, only the first 5 bytes. Notice how the format of the body differs: first, the size of the chunk is sent, followed by the content of the chunk itself. At the end of each chunk, the server adds a ␍␊ sequence.</p>

<p>Now, let's consider receiving the second chunk. How might that appear?<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>HTTP/1.1 200 OK␍␊
Date: Mon, 27 Jul 2009 12:28:53 GMT␍␊
Transfer-Encoding: chunked␍␊
Content-Type: text/plain␍␊
␍␊
5␍␊
Hello␍␊
7␍␊
 World!␍␊
</code></pre>

</div>



<p>We've received an additional 7 bytes of the response. But what transpired between <code>Hello␍␊</code> and <code>7␍␊</code>? How was the response processed in that interval? Imagine that before sending the 7, the server took 10 seconds pondering the next word. If you were to inspect the Network tab of your browser's Developer Tools during this pause, you'd see the response from the server had started and remained "in progress" throughout these 10 seconds. This is because the server hadn't indicated the end of the response.</p>

<p>So, how does the browser determine when the response should be treated as "completed"? There's a convention for that. The server must send a <code>0␍␊␍␊</code> sequence. In layman's terms, it's saying, "I'm sending you a chunk that has zero length, signifying there's nothing more to come." In the Network tab, this sequence will mark the moment the request has concluded.<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>HTTP/1.1 200 OK␍␊
Date: Mon, 27 Jul 2009 12:28:53 GMT␍␊
Transfer-Encoding: chunked␍␊
Content-Type: text/plain␍␊
␍␊
5␍␊
Hello␍␊
7␍␊
 World!␍␊
0␍␊
␍␊
</code></pre>

</div>



<h3>
  
  
  The Nuances of HTTP Transmission
</h3>

<p>In the realm of HTTP headers, understanding the distinction between <code>Content-Length: &lt;number&gt;</code> and <code>Transfer-Encoding: chunked</code> is crucial. At a first glance, <code>Content-Length: &lt;number&gt;</code> might suggest that data isn't streamed, but this isn't entirely accurate. While it's true that this header indicates the total length of the data to be received, it doesn't imply that data is transmitted as a single massive chunk. Underneath the HTTP layer, protocols like TCP/IP dictate the actual transmission mechanics, which inherently involves breaking data down into smaller packets. So, while the <code>Content-Length</code> header gives a system the signal that once it accumulates the specified amount of data, it's ready for rendering, the actual data transfer is executed incrementally at a lower level. Some contemporary browsers, capitalizing on this inherent packetization, initiate the rendering process even before the entire data is received. This is particularly beneficial for specific data formats that lend themselves to progressive rendering. On the other hand, the <code>Transfer-Encoding: chunked</code> header offers more explicit control over data streaming at the HTTP level, marking each chunk of data as it's sent. This provides even more flexibility, especially for dynamically generated content or when the full content length is unknown at the outset.</p>

<h3>
  
  
  <code>&lt;Suspense /&gt;</code>
</h3>

<p>Alright, now we've grasped one foundational concept that's pivotal for Component Streaming in Next.js. Before delving into <code>&lt;Suspense /&gt;</code>, let's first articulate the problem it addresses. Sometimes, seeing is more instructive than a lengthy explanation. So, let's craft a helper function for illustration:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight tsx"><code><span class="k">export</span> <span class="kd">function</span> <span class="nx">wait</span><span class="o">&lt;</span><span class="nx">T</span><span class="o">&gt;</span><span class="p">(</span><span class="nx">ms</span><span class="p">:</span> <span class="kr">number</span><span class="p">,</span> <span class="nx">data</span><span class="p">:</span> <span class="nx">T</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span> <span class="k">new</span> <span class="nb">Promise</span><span class="o">&lt;</span><span class="nx">T</span><span class="o">&gt;</span><span class="p">((</span><span class="nx">resolve</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="nx">setTimeout</span><span class="p">(()</span> <span class="o">=&gt;</span> <span class="nx">resolve</span><span class="p">(</span><span class="nx">data</span><span class="p">),</span> <span class="nx">ms</span><span class="p">);</span>
  <span class="p">});</span>
<span class="p">}</span>
</code></pre>

</div>



<p>This function will assist us in simulating exceedingly prolonged, fake requests.</p>

<p>To start, initialize a Next.js app using npx <a href="mailto:create-next-app@latest">create-next-app@latest</a>. Clear out any unnecessary elements, and paste the following code into <code>app/page.tsx</code>:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight tsx"><code><span class="k">import</span> <span class="p">{</span> <span class="nx">wait</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">@/helpers/wait</span><span class="dl">"</span><span class="p">;</span>

<span class="kd">const</span> <span class="nx">MyComponent</span> <span class="o">=</span> <span class="k">async</span> <span class="p">()</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="kd">const</span> <span class="nx">data</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">wait</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="p">{</span> <span class="na">name</span><span class="p">:</span> <span class="dl">"</span><span class="s2">Denis</span><span class="dl">"</span> <span class="p">});</span>
  <span class="k">return</span> <span class="p">&lt;</span><span class="nt">p</span><span class="p">&gt;</span><span class="si">{</span><span class="nx">data</span><span class="p">.</span><span class="nx">name</span><span class="si">}</span><span class="p">&lt;/</span><span class="nt">p</span><span class="p">&gt;;</span>
<span class="p">};</span>

<span class="k">export</span> <span class="kd">const</span> <span class="nx">dynamic</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">force-dynamic</span><span class="dl">"</span><span class="p">;</span>

<span class="k">export</span> <span class="k">default</span> <span class="k">async</span> <span class="kd">function</span> <span class="nx">Home</span><span class="p">()</span> <span class="p">{</span>
  <span class="k">return</span> <span class="p">(</span>
    <span class="p">&lt;&gt;</span>
      <span class="p">&lt;</span><span class="nt">p</span><span class="p">&gt;</span>Some text<span class="p">&lt;/</span><span class="nt">p</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nc">MyComponent</span> <span class="p">/&gt;</span>
    <span class="p">&lt;/&gt;</span>
  <span class="p">);</span>
<span class="p">}</span>
</code></pre>

</div>



<p>This structure provides a simple page layout: a text block containing “Some text” and a component that waits for 10 seconds before outputting the data.</p>

<p>Now, execute <code>npm run build &amp;&amp; npm run start</code> followed by a <code>curl localhost:3000</code> (or try to open it in a browser) command. What do we observe?</p>

<p>We experience a delay of 10 seconds before receiving the entire page content, including both “Some text” and “Denis”. For users, this means they won't be able to view the “Some text” content while <code>&lt;MyComponent /&gt;</code> is fetching its data. This is far from ideal; the browser tab's spinner would keep spinning for a solid 10 seconds before displaying any content to the user.</p>

<p>However, by wrapping our component with the <code>&lt;Suspense/&gt;</code> tag and trying again, we observe an instantaneous response. Let's delve into this method. We encase our component in <code>&lt;Suspense&gt;</code> and also assign a fallback prop with the value "We are loading...".<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight tsx"><code><span class="k">export</span> <span class="k">default</span> <span class="k">async</span> <span class="kd">function</span> <span class="nx">Home</span><span class="p">()</span> <span class="p">{</span>
  <span class="k">return</span> <span class="p">(</span>
    <span class="p">&lt;&gt;</span>
      <span class="p">&lt;</span><span class="nt">p</span><span class="p">&gt;</span>Some text<span class="p">&lt;/</span><span class="nt">p</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nc">Suspense</span> <span class="na">fallback</span><span class="p">=</span><span class="si">{</span><span class="dl">"</span><span class="s2">We are loading...</span><span class="dl">"</span><span class="si">}</span><span class="p">&gt;</span>
        <span class="p">&lt;</span><span class="nc">MyComponent</span> <span class="p">/&gt;</span>
      <span class="p">&lt;/</span><span class="nc">Suspense</span><span class="p">&gt;</span>
    <span class="p">&lt;/&gt;</span>
  <span class="p">);</span>
<span class="p">}</span>
</code></pre>

</div>



<p>Now let us open it in a browser.<br>
<a href="https://res.cloudinary.com/practicaldev/image/fetch/s--oCXP9oNI--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5rm7w4l9x17v00u0v51f.png" class="article-body-image-wrapper"><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--oCXP9oNI--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5rm7w4l9x17v00u0v51f.png" alt="When you inspect the Network tab in DevTools, you'll observe that the server's response is still ongoing or &quot;hasn't yet completed.&quot; Examining the &quot;Response Headers&quot; section of the request, you'll find the &quot;Transfer-Encoding: chunked&quot; entry." width="800" height="200"></a></p>

<p>Now, we observe that the string provided as the fallback prop for <code>&lt;Suspense /&gt;</code> temporarily stands in for the <code>&lt;MyComponent /&gt;</code>. After the 10-second wait, we're then presented with the actual content. Let's scrutinize the HTML response we've received.<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight html"><code><span class="cp">&lt;!DOCTYPE html&gt;</span>
<span class="nt">&lt;html</span> <span class="na">lang=</span><span class="s">"en"</span><span class="nt">&gt;</span>
<span class="nt">&lt;head&gt;</span>
    <span class="c">&lt;!-- Omitted --&gt;</span>
<span class="nt">&lt;/head&gt;</span>
<span class="nt">&lt;body</span> <span class="na">class=</span><span class="s">"__className_20951f"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;p&gt;</span>Some text<span class="nt">&lt;/p&gt;</span><span class="c">&lt;!--$?--&gt;</span>
    <span class="nt">&lt;template</span> <span class="na">id=</span><span class="s">"B:0"</span><span class="nt">&gt;&lt;/template&gt;</span>
    Waiting for MyComponent...<span class="c">&lt;!--/$--&gt;</span>
    <span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"/_next/static/chunks/webpack-f0069ae2f14f3de1.js"</span> <span class="na">async=</span><span class="s">""</span><span class="nt">&gt;&lt;/script&gt;</span>
    <span class="nt">&lt;script&gt;</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nx">__next_f</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">__next_f</span> <span class="o">||</span> <span class="p">[]).</span><span class="nx">push</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span><span class="nt">&lt;/script&gt;</span>
    <span class="nt">&lt;script&gt;</span><span class="nb">self</span><span class="p">.</span><span class="nx">__next_f</span><span class="p">.</span><span class="nx">push</span><span class="p">(</span><span class="cm">/* Omitted */</span><span class="p">)</span><span class="nt">&lt;/script&gt;</span>
    <span class="nt">&lt;script&gt;</span><span class="nb">self</span><span class="p">.</span><span class="nx">__next_f</span><span class="p">.</span><span class="nx">push</span><span class="p">(</span><span class="cm">/* Omitted */</span><span class="p">)</span><span class="nt">&lt;/script&gt;</span>
    <span class="nt">&lt;script&gt;</span><span class="nb">self</span><span class="p">.</span><span class="nx">__next_f</span><span class="p">.</span><span class="nx">push</span><span class="p">(</span><span class="cm">/* Omitted */</span><span class="p">)</span><span class="nt">&lt;/script&gt;</span>
    <span class="nt">&lt;script&gt;</span><span class="nb">self</span><span class="p">.</span><span class="nx">__next_f</span><span class="p">.</span><span class="nx">push</span><span class="p">(</span><span class="cm">/* We haven't received a chunk that closes this tag...
</span></code></pre>

</div>



<p>While we haven't yet received the complete page, we can already view its content in the browser. But why is that possible? This behavior is due to the error tolerance of modern browsers. Consider a scenario where you visit a website, but because a developer forgot to close a tag, the site doesn't display correctly. Although browser developers could enforce strict error-free HTML, such a decision would degrade the user experience. As users, we expect web pages to load and display their content, regardless of minor errors in the underlying code. To ensure this, browsers implement numerous mechanisms under the hood to compensate for such issues. For instance, if there's an opened </p> tag that hasn't been closed, the browser will automatically "close" it. This is done in an effort to deliver the best possible viewing experience, even when faced with imperfect HTML.

<p>And it's evident that Next capitalizes on this inherent browser behavior when implementing Component Streaming. By pushing chunks of content as they become available and leveraging browsers' ability to interpret and render partial or even slightly malformed content, Next.js ensures faster perceived load times and enhances user experience.</p>

<p>The strength of this approach lies in its alignment with the realities of web browsing. Users generally prefer immediate feedback, even if it's incremental, over waiting for an entire page to load. By sending parts of a page as soon as they're ready, Next.js optimally meets this preference.</p>

<p>Now, observe this segment:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight html"><code><span class="c">&lt;!--$?--&gt;</span>
  <span class="nt">&lt;template</span> <span class="na">id=</span><span class="s">"B:0"</span><span class="nt">&gt;&lt;/template&gt;</span>
  Waiting for MyComponent...
<span class="c">&lt;!--/$--&gt;</span>
</code></pre>

</div>



<p>We can spot our placeholder text adjacent to an empty <code>&lt;template&gt;</code> tag bearing the <code>B:0</code> id. Further, we can discern that the response from <code>localhost:3000</code> is still underway. The trailing script tag remains unclosed. Next.js uses a placeholder template to make room for forthcoming HTML that will be populated with the next chunk.</p>

<p>After the next chunk has arrived, we now have the following markup (I’ve added some newlines to make it more readable)...</p>

<p><em>Don't attempt to unminify the code of the $RC function in your head. This is the <code>completeBoundary</code> function, and you can find a commented version <a href="https://github.com/facebook/react/blob/b9be4537c2459f8fc0312b796570003620bc8600/packages/react-dom-bindings/src/server/fizz-instruction-set/ReactDOMFizzInstructionSetShared.js#L46">here</a>.</em><br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight html"><code><span class="nt">&lt;p&gt;</span>Some text<span class="nt">&lt;/p&gt;</span>

<span class="c">&lt;!--$?--&gt;</span>
<span class="nt">&lt;template</span> <span class="na">id=</span><span class="s">"B:0"</span><span class="nt">&gt;&lt;/template&gt;</span>
Waiting for MyComponent...
<span class="c">&lt;!--/$--&gt;</span>

<span class="c">&lt;!-- &lt;script&gt; tags omitted --&gt;</span>

<span class="nt">&lt;div</span> <span class="na">hidden</span> <span class="na">id=</span><span class="s">"S:0"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;p&gt;</span>Denis<span class="nt">&lt;/p&gt;</span>
<span class="nt">&lt;/div&gt;</span>

<span class="nt">&lt;script&gt;</span>
  <span class="nx">$RC</span> <span class="o">=</span> <span class="kd">function</span> <span class="p">(</span><span class="nx">b</span><span class="p">,</span> <span class="nx">c</span><span class="p">,</span> <span class="nx">e</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">c</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">getElementById</span><span class="p">(</span><span class="nx">c</span><span class="p">);</span>
    <span class="nx">c</span><span class="p">.</span><span class="nx">parentNode</span><span class="p">.</span><span class="nx">removeChild</span><span class="p">(</span><span class="nx">c</span><span class="p">);</span>
    <span class="kd">var</span> <span class="nx">a</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">getElementById</span><span class="p">(</span><span class="nx">b</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="nx">a</span><span class="p">)</span> <span class="p">{</span>
      <span class="nx">b</span> <span class="o">=</span> <span class="nx">a</span><span class="p">.</span><span class="nx">previousSibling</span><span class="p">;</span>
      <span class="k">if</span> <span class="p">(</span><span class="nx">e</span><span class="p">)</span>
        <span class="nx">b</span><span class="p">.</span><span class="nx">data</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">$!</span><span class="dl">"</span><span class="p">,</span>
          <span class="nx">a</span><span class="p">.</span><span class="nx">setAttribute</span><span class="p">(</span><span class="dl">"</span><span class="s2">data-dgst</span><span class="dl">"</span><span class="p">,</span> <span class="nx">e</span><span class="p">);</span>
      <span class="k">else</span> <span class="p">{</span>
        <span class="nx">e</span> <span class="o">=</span> <span class="nx">b</span><span class="p">.</span><span class="nx">parentNode</span><span class="p">;</span>
        <span class="nx">a</span> <span class="o">=</span> <span class="nx">b</span><span class="p">.</span><span class="nx">nextSibling</span><span class="p">;</span>
        <span class="kd">var</span> <span class="nx">f</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="k">do</span> <span class="p">{</span>
          <span class="k">if</span> <span class="p">(</span><span class="nx">a</span> <span class="o">&amp;&amp;</span> <span class="mi">8</span> <span class="o">===</span> <span class="nx">a</span><span class="p">.</span><span class="nx">nodeType</span><span class="p">)</span> <span class="p">{</span>
            <span class="kd">var</span> <span class="nx">d</span> <span class="o">=</span> <span class="nx">a</span><span class="p">.</span><span class="nx">data</span><span class="p">;</span>
            <span class="k">if</span> <span class="p">(</span><span class="dl">"</span><span class="s2">/$</span><span class="dl">"</span> <span class="o">===</span> <span class="nx">d</span><span class="p">)</span>
              <span class="k">if</span> <span class="p">(</span><span class="mi">0</span> <span class="o">===</span> <span class="nx">f</span><span class="p">)</span>
                <span class="k">break</span><span class="p">;</span>
              <span class="k">else</span>
                <span class="nx">f</span><span class="o">--</span><span class="p">;</span>
            <span class="k">else</span>
              <span class="dl">"</span><span class="s2">$</span><span class="dl">"</span> <span class="o">!==</span> <span class="nx">d</span> <span class="o">&amp;&amp;</span> <span class="dl">"</span><span class="s2">$?</span><span class="dl">"</span> <span class="o">!==</span> <span class="nx">d</span> <span class="o">&amp;&amp;</span> <span class="dl">"</span><span class="s2">$!</span><span class="dl">"</span> <span class="o">!==</span> <span class="nx">d</span> <span class="o">||</span> <span class="nx">f</span><span class="o">++</span>
          <span class="p">}</span>
          <span class="nx">d</span> <span class="o">=</span> <span class="nx">a</span><span class="p">.</span><span class="nx">nextSibling</span><span class="p">;</span>
          <span class="nx">e</span><span class="p">.</span><span class="nx">removeChild</span><span class="p">(</span><span class="nx">a</span><span class="p">);</span>
          <span class="nx">a</span> <span class="o">=</span> <span class="nx">d</span>
        <span class="p">}</span> <span class="k">while</span> <span class="p">(</span><span class="nx">a</span><span class="p">);</span>
        <span class="k">for</span> <span class="p">(;</span> <span class="nx">c</span><span class="p">.</span><span class="nx">firstChild</span><span class="p">;)</span>
          <span class="nx">e</span><span class="p">.</span><span class="nx">insertBefore</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">firstChild</span><span class="p">,</span> <span class="nx">a</span><span class="p">);</span>
        <span class="nx">b</span><span class="p">.</span><span class="nx">data</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">$</span><span class="dl">"</span>
      <span class="p">}</span>
      <span class="nx">b</span><span class="p">.</span><span class="nx">_reactRetry</span> <span class="o">&amp;&amp;</span> <span class="nx">b</span><span class="p">.</span><span class="nx">_reactRetry</span><span class="p">()</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="p">;</span>
  <span class="nx">$RC</span><span class="p">(</span><span class="dl">"</span><span class="s2">B:0</span><span class="dl">"</span><span class="p">,</span> <span class="dl">"</span><span class="s2">S:0</span><span class="dl">"</span><span class="p">)</span>
<span class="nt">&lt;/script&gt;</span>
</code></pre>

</div>



<p>We receive a hidden <code>&lt;div&gt;</code> with the <code>id="S:0"</code>. This contains the markup for <code>&lt;MyComponent /&gt;</code>. Alongside this, we are presented with an intriguing script that defines a global variable, <code>$RC</code>. This variable references a function that performs some operations with <code>getElementById</code> and <code>insertBefore</code>.</p>

<p>The concluding statement in the script, <code>$RC("B:0", "S:0")</code>, invokes the aforementioned function and supplies <code>"B:0"</code> and <code>"S:0"</code> as arguments. As we've deduced, <code>B:0</code> corresponds to the ID of the template that previously held our fallback. Concurrently, <code>S:0</code> matches the ID of the newly acquired <code>&lt;div&gt;</code>. To distill this information, the <code>$RC</code> function essentially instructs: "Retrieve the markup from the S:0 div and position it where the <code>B:0</code> template resides."</p>

<p>Let's refine that for clarity:</p>

<ol>
<li>
<strong>Initiating the Chunked Transfer</strong>: Next.js begins by sending the Transfer-Encoding: chunked header, signaling the browser that the response length is undetermined at this stage.</li>
<li>
<strong>Executing Home Page</strong>: As the Home page executes, it encounters no await operations. This means no data fetching is blocking the response from being sent immediately.</li>
<li>
<strong>Handling the Suspense</strong>: Upon reaching the  tag, it uses the fallback value for immediate rendering, while also inserting a placeholder <code>&lt;template /&gt;</code> tag. This will be used later to insert the actual HTML once it's ready.</li>
<li>
<strong>Initial Response to the Browser</strong>: What's been rendered so far is sent to the browser. Yet, the "0␍␊␍␊" sequence hasn't been sent, indicating the browser should expect more data to come.</li>
<li>
<strong>Component Data Request</strong>: The server communicates with MyComponent, requesting its data and essentially saying, "We need your content, let us know when you're ready."</li>
<li>
<strong>Component Rendering</strong>: After MyComponent fetches its data, it renders and produces the corresponding HTML.</li>
<li>
<strong>Sending the Component's HTML</strong>: This HTML is then sent to the browser as a new chunk.</li>
<li>
<strong>JavaScript Attachment</strong>: The browser's JavaScript then appends this new chunk of HTML to the previously placed  tag from step #3.</li>
<li>
<strong>Termination Sequence</strong>: Finally, the server sends the termination sequence, signaling the end of the response.</li>
</ol>

<h3>
  
  
  Diving into Multiple <code>&lt;Suspense /&gt;</code>
</h3>

<p>Handling a singular <code>&lt;Suspense /&gt;</code> tag is straightforward, but what if a page has multiple of them? How does Next.js cope with this situation? Interestingly, the core approach doesn't deviate much. Here's what changes when managing multiple <code>&lt;Suspense /&gt;</code> tags:</p>

<p><strong>Fallbacks at the Forefront</strong>: Each <code>&lt;Suspense /&gt;</code> tag comes equipped with its own fallback. During the rendering phase, all these fallback values are leveraged simultaneously, ensuring that every suspended component offers a provisional visual cue to the user. This is an extension of the third point from our previous list.</p>

<p><strong>Unified Request for Content</strong>: Just as with a single <code>&lt;Suspense /&gt;</code>, Next.js sends out a unified call to all components wrapped within the <code>&lt;Suspense /&gt;</code> tags. It's essentially broadcasting, "Provide your content as soon as you're ready."</p>

<p><strong>Waiting for All Components</strong>: The termination sequence is of utmost importance, signaling the end of a response. However, in cases with multiple <code>&lt;Suspense /&gt;</code> tags, the termination sequence is held back until every single component has sent its content. This ensures that the browser knows to expect, and subsequently render, the content from all components, providing a holistic page-view to the end user.</p>

<p>The advent of features like <code>&lt;Suspense /&gt;</code> in Next.js underscores the framework's dedication to enhancing user experience. By tapping into the innate behavior of browsers and optimizing content delivery, Next.js ensures users encounter minimal wait times and see content as swiftly as possible. This deep dive into the inner workings of component streaming and chunked transfer encoding reveals the intricate dance of protocols, rendering, and real-time adjustments that takes place behind the scenes. As web developers, understanding these nuances not only makes us better at our craft but also equips us to deliver seamless and responsive digital experiences for our users. Embrace the future of web development with Next.js, where efficiency meets elegance.</p>

 </details> 
 <hr /> 

 #### - [My Programming Journey So Far](https://dev.to/manishmehra/my-programming-journey-so-far-50oh) 
 <details><summary>Article</summary> <p>By mid-2020, the school had ended. All things were in captivity: the covid pandemic. The year passed in the blink of an eye. By the end of 2020, I enrolled in a three-year computer-related degree. I knew programming but didn’t know no more than printing Hello World.</p>

<p>Starting in 2021, I began my programming journey with CS50, a popular, free introductory course on computer science and programming. The course gave me a good overview and exposed me to concepts like memory and algorithms like binary search(which I will forget soon).</p>

<p>However, I avoided further learning the hard parts of computer science and focused on advancing toward the big picture. The big picture was to build functional and interactive websites for myself. That was my primary focus, and it has evolved somewhat since then. Nonetheless, this goal was sufficient to keep me motivated.</p>

<p>After this course, I spent just a week or two learning HTML and then dedicated the next two to three months to the challenging task of centering the div, which, to this day, I find one of the most challenging aspects of web development.</p>

<p>I was excited to learn Javascript after this.</p>

<p>My first encounter with JavaScript was strange. In CS50(C language), I have known about certain data types like int, char, and bool. You can only store these kinds of values in your variable. But in Javascript, I was assigning my variable to something like a dom node. I was dumbfounded.<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>const button =  document.getElementById("#button")
</code></pre>

</div>



<p>I couldn’t wrap my head around it. So I proceeded with memorizing, believing it to be a rule allowed in JavaScript.</p>

<p>Two to three months later, I could build a simple to-do list with JavaScript, display and hide modal, and make a navbar slide.</p>

<h2>
  
  
  Learning React.js
</h2>

<p>React Js came into my life. Some called it a library, some a framework, and some were confused. Angular JS was its competitor, the awful frontend framework made by Google, as proclaimed. And Vue Js was also around the corner at that time. Despite all the confusion, I went with React JS and not the hated Angular.</p>

<p>Initially, React Js was confusing. Till now, I have been working on one file with javascript. With React Js, there are many files and folders. It took me 2-3 months to grasp this new environment, the naming conventions, and the use of the terminal and dependency system.</p>

<p>Eventually, it all started to make sense. And I could build a simple UI with React, which was unfathomably painful with JavaScript alone.</p>

<h2>
  
  
  Building Things Together
</h2>

<p>I joined a Discord server for programming by the end of 2021. Though most people were writing their first line of code, few surpassed that beginner's phase and were looking for opportunities to grow their skills. I saw messages asking to collaborate on projects. I responded to one of the random messages offering my aid in building the project. It turns out to be a good learning opportunity later.</p>

<p>In the team, there were two scrawny-looking college kids. Both were passionate and took the self-taught route. Before joining me, the project was set up with React Js on Frontend and Firebase as backend/database.</p>

<p>The project was a research papers website, where users can log in and download the papers. We built the project over two months. It was fun. We explored tools like git, project management tools, and wireframing software like Adobe XD.</p>

<p>After this project, we collaborated on another project where we decided to build our backend. Since we already knew Javascript, learning the backend seemed less overwhelming. With Node.js/express, we can easily set up a server and create APIs for our react app to consume. The popular database choice with node/express app was MongoDB. We moved with it. MongoDB turned out to be easy to get started.</p>

<p>After working on this project for a few months on and off, this project went nowhere. The cause of failure was our vague requirements. Continuing the project felt like a meaningless pursuit. So we dropped it. In the end, we lost nothing and gained a few more skills.</p>

<h2>
  
  
  The Work Experience
</h2>

<p>After a year and a half of learning to program(2022), I stumbled across an opportunity to work for a small startup as a React Developer. This opportunity was offered to me by someone from an online community where we occasionally discuss books, ideas, arts, and things in general. This person referred me, and the next day, I received a call from the energetic CEO. We talked about general things. At the end, he asked if I could sit for the interview. I couldn't say no. After a few days, the senior developer of the team interviewed me.</p>

<p>The questions revolved around React Js. The interview lasted no more than 10 minutes. I was expected only to know basic things like how useEffect hook works and styling components. I was stunned when the interview ended early and thought I missed the mark. </p>

<p>The next day, I got a call. The team welcomed me. </p>

<p>After a few days, I was staring at the massive code base. It wasn’t basic as I guessed from the interview. It terrified me, and I was fool enough to suggest to my senior that we needed to rewrite the whole codebase with Next JS. The excuse I made was terrible SEO because it was Single Page App. The suggestion felt reasonable, at least to me, since our website needed to rank higher on Google. The senior explained that this optimization is not required as we are still in the initial phase. Our primary focus is to build the features and improve functionality instead of doing the optimizations. And the codebase was only one year old and has worked for the requirement so far.</p>

<p>I got comfortable with the codebase over the next few weeks by making small changes, slowly fixing bugs, adding functionality, and pushing design changes. I realized I didn't have to know the whole codebase. I only need to understand the piece of code related to the bug, feature, or improvement I was dealing with.</p>

<p>Our team was remote, so I had the flexibility to attend my college when it was required. I moved to the office for the next few months and work fully onsite. The office was in a different state(Chennai, India), a distance of more than 1,700km(1,000 miles) from my state(Delhi, India). Moreover, the language, culture, and food were different. Despite I moved. I was excited about the opportunity to work closely with the product owners, and it was a good travel opportunity.</p>

<p>I returned after three months and continued working till my graduation. During all this time, I learned a lot from my senior developers, how building software works in the real world, working with a team, effectively communicating my thoughts, and the list is endless. I was grateful for the opportunity. The people I worked with were chilled, supportive, and humble. There could not have been a better place for me to kickstart my career.</p>

<p>In July 2023, after ten months of working, I decided to take a break from work and quit this job. I also just graduated. My motives for the break were to fill those gaps in my knowledge that I was painfully aware of. And to take some time to plan my career and start fresh again in a new place.</p>




<p>It’s been two months since I left the job and the graduation. I've started learning a new language(Java), studying data structures and algorithms(I’m very inconsistent here), and revisiting the Javascript fundamentals. Also, a few days ago, I picked a book called The Pragmatic Programmer.</p>

<p>In all my attempts, it barely feels like progress is being made. It's a strange phase. I’m not a beginner, or an intermediate or advanced. It's more like starting anew, and the process is painfully slow and monotonous. </p>

<p>Learning is great, but the reality is I can't dedicate myself full-time to it. It would be fantastic if I could, but I need to manage my finances. Without a stable income, I can't sustain this programming journey.</p>

<p>So, a few days ago I decided to apply for the junior role in my local and if I get the job, I’ll have to manage my time and continue learning things on the side.</p>

<p>Till now, I’ve gotten 2-3 rejections with no interviews out of the 50+ applications I applied and the rest haven’t responded. It feels worse than rejection. Most jobs require a minimum of 4-5 years of experience, and all the junior-level posts have applicants from 500 to 3k. It’s overwhelming and hopeless to see, but that’s the current state of affairs.</p>

<p>I also realized my resume is not up to the mark. Other than 10 months of experience. I don’t have good projects to showcase my skills. There's this basic node js web scraper that fetches the news headlines from different sites, and my react app consumes the API and shows these news headlines.</p>

<p>I’ll keep applying and continue learning things.</p>

<h2>
  
  
  Final Thoughts
</h2>

<p>When I was learning to program back then, the big picture that kept me motivated was the idea of building websites for myself. Now, I can easily create a website. This big picture has evolved. Building things is great, and programming gives you the ability to do so. However, building useful, reliable things is what I'm striving for now. To achieve that, I must hone my craft and improve. I know I'm still far from my goal, but I'll continue working to get better every day. </p>

 </details> 
 <hr /> 
<!-- BLOG-POST-LIST:END -->
</table>
</details>


<!-- TODO
Change the 3stats boxes around, possibly two on top and one on bottom
Fix RSSfeed
Fix Spotify Playlists
Fix Socials [Portfolio, Discord, Linkedin]
In the future, add Public Repositories of Selected Projects
-->
